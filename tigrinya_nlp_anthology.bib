@inproceedings{abate-etal-2018-parallel-corpora,
  title     = {Parallel Corpora for bi-lingual English-Ethiopian Languages Statistical Machine Translation},
  author    = {Solomon Teferra Abate and Michael Melese Woldeyohannis and Martha Yifiru Tachbelie and Million Meshesha and Solomon Atinafu and Wondwossen Mulugeta and Yaregal Assabie and Hafte Abera and Binyam Ephrem Seyoum and Tewodros Abebe and Wondimagegnhue Tsegaye and Amanuel Lemma and Tsegaye Andargie and Seifedin Shifaw},
  booktitle = {International Conference on Computational Linguistics},
  year      = {2018},
  tasks     = {Machine Translation},
  abstract  = {In this paper, we describe an attempt towards the development of parallel corpora for English and Ethiopian Languages, such as Amharic, Tigrigna, Afan-Oromo, Wolaytta and Ge’ez. The corpora are used for conducting a bi-directional statistical machine translation experiments. The BLEU scores of the bi-directional Statistical Machine Translation (SMT) systems show a promising result. The morphological richness of the Ethiopian languages has a great impact on the performance of SMT specially when the targets are Ethiopian languages. Now we are working towards an optimal alignment for a bi-directional English-Ethiopian languages SMT.}
}

@inproceedings{abate-etal-2019-english-ethiopian-smt,
  title     = {English-Ethiopian Languages Statistical Machine Translation},
  author    = {Solomon Teferra Abate and Michael Melese and Martha Yifiru Tachbelie and Million Meshesha and Solomon Atinafu and Wondwossen Mulugeta and Yaregal Assabie and Hafte Abera and Biniyam Ephrem and Tewodros Gebreselassie and Wondimagegnhue Tsegaye Tufa and Amanuel Lemma and Tsegaye Andargie and Seifedin Shifaw},
  booktitle = {WNLP Workshop at ACL},
  year      = {2019},
  tasks     = {Machine Translation},
  abstract  = {In this paper, we describe an attempt towards the development of parallel corpora for English and Ethiopian Languages, such as Amharic, Tigrigna, Afan-Oromo, Wolaytta and Ge’ez. The corpora are used for conducting bi-directional SMT experiments. The BLEU scores of the bi-directional SMT systems show a promising result. The morphological richness of the Ethiopian languages has a great impact on the performance of SMT especially when the targets are Ethiopian languages.}
}

@inproceedings{abdelkadir-etal-2023-mt-error,
  title     = {Error Analysis of {Tigrinya - English} Machine Translation Systems},
  author    = {Nuredin Ali Abdelkadir and Negasi Haile Abadi and Asmelash Teka Hadgu},
  booktitle = {Proceedings of the 4th Workshop on African Natural Language Processing, AfricaNLP@ICLR 2023, Kigali, Rwanda, May 1, 2023},
  year      = {2023},
  url       = {https://openreview.net/pdf?id=BQVqNyzCxx},
  tasks     = {Machine Translation},
  timestamp = {Wed, 06 Sep 2023 12:12:32 +0200}
}

@inproceedings{abera-h-mariam-2018-design,
  title     = {Design of a {T}igrinya Language Speech Corpus for Speech Recognition},
  author    = {Abera, Hafte  and H/Mariam, Sebsibe},
  booktitle = {Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing},
  month     = {August},
  year      = {2018},
  publisher = {Association for Computational Linguistics},
  pages     = {78--82},
  address   = {Santa Fe, New Mexico, USA},
  url       = {https://aclanthology.org/W18-3811},
  tasks     = {Speech Recognition},
  abstract  = {In this paper, we describe the first Tigrinya Languages speech corpora designed and development for speech recognition purposes. Tigrinya, often written as Tigrigna (ትግርኛ) /tɪˈɡrinjə/ belongs to the Semitic branch of the Afro-Asiatic languages where it shows the characteristic features of a Semitic language. It is spoken by ethnic Tigray-Tigrigna people in the Horn of Africa. The paper outlines different corpus designing process analysis of related work on speech corpora creation for different languages. The authors provide also procedures that were used for the creation of Tigrinya speech recognition corpus which is the under-resourced language. One hundred and thirty speakers, native to Tigrinya language, were recorded for training and test dataset set. Each speaker read 100 texts, which consisted of syllabically rich and balanced sentences. Ten thousand sets of sentences were used to prompt sheets. These sentences contained all of the contextual syllables and phones.}
}

@inproceedings{abera-h-mariam-2019-speech,
  title     = {Speech Recognition for {T}igrinya language Using Deep Neural Network Approach},
  author    = {Abera, Hafte  and H/mariam, Sebsibe},
  booktitle = {Proceedings of the 2019 Workshop on Widening NLP},
  month     = {August},
  year      = {2019},
  publisher = {Association for Computational Linguistics},
  pages     = {7--9},
  address   = {Florence, Italy},
  url       = {https://aclanthology.org/W19-3603},
  tasks     = {Speech Recognition},
  abstract  = {This work presents a speech recognition model for Tigrinya language .The Deep Neural Network is used to make the recognition model. The Long Short-Term Memory Network (LSTM), which is a special kind of Recurrent Neural Network composed of Long Short-Term Memory blocks, is the primary layer of our neural network model. The 40-dimensional features are MFCC-LDA-MLLT-fMLLR with CMN were used. The acoustic models are trained on features that are obtained by projecting down to 40 dimensions using linear discriminant analysis (LDA). Moreover, speaker adaptive training (SAT) is done using a single feature-space maximum likelihood linear regression (FMLLR) transform estimated per speaker. We train and compare LSTM and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models. Finally, the accuracy of the model is evaluated based on the recognition rate.}
}

@inproceedings{adelani-etal-2023-masakhanews,
  title     = {{M}asakha{NEWS}: News Topic Classification for {A}frican languages},
  author    = {Adelani, David Ifeoluwa  and Masiak, Marek  and Azime, Israel Abebe  and Alabi, Jesujoba  and Tonja, Atnafu Lambebo  and Mwase, Christine  and Ogundepo, Odunayo  and Dossou, Bonaventure F. P.  and Oladipo, Akintunde  and Nixdorf, Doreen  and Emezue, Chris Chinenye  and Al-azzawi, Sana  and Sibanda, Blessing  and David, Davis  and Ndolela, Lolwethu  and Mukiibi, Jonathan  and Ajayi, Tunde  and Moteu, Tatiana  and Odhiambo, Brian  and Owodunni, Abraham  and Obiefuna, Nnaemeka  and Mohamed, Muhidin  and Muhammad, Shamsuddeen Hassan  and Ababu, Teshome Mulugeta  and Salahudeen, Saheed Abdullahi  and Yigezu, Mesay Gemeda  and Gwadabe, Tajuddeen  and Abdulmumin, Idris  and Taye, Mahlet  and Awoyomi, Oluwabusayo  and Shode, Iyanuoluwa  and Adelani, Tolulope  and Abdulganiyu, Habiba  and Omotayo, Abdul-Hakeem  and Adeeko, Adetola  and Afolabi, Abeeb  and Aremu, Anuoluwapo  and Samuel, Olanrewaju  and Siro, Clemencia  and Kimotho, Wangari  and Ogbu, Onyekachi  and Mbonu, Chinedu  and Chukwuneke, Chiamaka  and Fanijo, Samuel  and Ojo, Jessica  and Awosan, Oyinkansola  and Kebede, Tadesse  and Sakayo, Toadoum Sari  and Nyatsine, Pamela  and Sidume, Freedmore  and Yousuf, Oreen  and Oduwole, Mardiyyah  and Tshinu, Kanda  and Kimanuka, Ussen  and Diko, Thina  and Nxakama, Siyanda  and Nigusse, Sinodos  and Johar, Abdulmejid  and Mohamed, Shafie  and Hassan, Fuad Mire  and Mehamed, Moges Ahmed  and Ngabire, Evrard  and Jules, Jules  and Ssenkungu, Ivan  and Stenetorp, Pontus},
  booktitle = {Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {November},
  year      = {2023},
  publisher = {Association for Computational Linguistics},
  pages     = {144--159},
  address   = {Nusa Dua, Bali},
  doi       = {10.18653/v1/2023.ijcnlp-main.10},
  url       = {https://aclanthology.org/2023.ijcnlp-main.10/},
  tasks     = {Topic Classification},
  abstract  = {African languages are severely under-represented in NLP research due to lack of datasets covering several NLP tasks. While there are individual language specific datasets that are being expanded to different tasks, only a handful of NLP tasks (e.g. named entity recognition and machine translation) have standardized benchmark datasets covering several geographical and typologically-diverse African languages. In this paper, we develop MasakhaNEWS -- a new benchmark dataset for news topic classification covering 16 languages widely spoken in Africa. We provide an evaluation of baseline models by training classical machine learning models and fine-tuning several language models. Furthermore, we explore several alternatives to full fine-tuning of language models that are better suited for zero-shot and few-shot learning such as cross-lingual parameter-efficient fine-tuning (like MAD-X), pattern exploiting training (PET), prompting language models (like ChatGPT), and prompt-free sentence transformer fine-tuning (SetFit and Cohere Embedding API). Our evaluation in zero-shot setting shows the potential of prompting ChatGPT for news topic classification in low-resource African languages, achieving an average performance of 70 F1 points without leveraging additional supervision like MAD-X. In few-shot setting, we show that with as little as 10 examples per label, we achieved more than 90\% (i.e. 86.0 F1 points) of the performance of full supervised training (92.6 F1 points) leveraging the PET approach.},
  editor    = {Park, Jong C.  and Arase, Yuki  and Hu, Baotian  and Lu, Wei  and Wijaya, Derry  and Purwarianti, Ayu  and Krisnadhi, Adila Alfa}
}

@mastersthesis{amare-2016-question,
  title    = {Tigrigna Question Answering System for Factoid Questions},
  author   = {Kibrom Haftu Amare},
  journal  = {Master's Thesis, submitted to Addis Ababa University},
  month    = {June},
  year     = {2016},
  school   = {Addis Ababa University},
  url      = {http://etd.aau.edu.et/handle/123456789/2375},
  tasks    = {Machine Translation, Question Answering},
  abstract = {Accessing relevant information is one of the major problems faced by Tigrigna language users for every domain of knowledge when dealing with huge amount of information especially in the Internet. Evidently, users are interested in obtaining a specific and precise answer to a specific question. However, obtaining a relevant and concise answer is a challenge to particular user question. For such situation, Tigrigna Question Answering system is a good solution. The proposed QA system comprises of question analysis, document analysis and answer extraction modules. The main function of question analysis module is taking a Tigrigna Question as input and then generates a query, expands a query and determines its Question Particle and Question Type. A statistical language model approach is used to model the classification of Tigrigna questions to their category or type. The document analysis module performs the process of pre-processing of parallel corpora, which are documents that contain question sentences in one document and answer sentences in another one, and also ranking and extracting answer contents. Answer extraction also performs the detail analysis on the retrieved answer contents based on the question type, question particle and query using the techniques of language modeling called Answer Model. This statistical language model does the extraction process of exact and precise Tigrigna answer in probabilistic manner from sets candidate answers. Generally, this system developed after reviewed literatures and related work, and selected the appropriate tools and data source such as Moses, GIZA++ and IRSTLM as tools and different Webs and Tigrigna newspapers and magazines as data sources. Our data sets are classified for training and testing activities of the system. Based on this, we collected around 1000 data sets for training and 200 data sets for testing. Performance evaluation conducted manually by comparing the system's answers with the answers exists in testing document, which is prepared for testing purpose. Finally the evaluation results of Tigrigna factoid QAS is expressed in terms of the average performance of a question type classifier which is 87\%, and the average Precision, Recall and F--measure of the answer extraction, precision is 88.5\%, recall is 85.9\% and F--measure is 87.2\%. Keywords: Tigrigna question answering, Tigrigna Factoid questions, Language model based question classification, question analysis, Document Analysis, Answer Extraction.}
}

@mastersthesis{asfaw-2018-lid,
  title    = {A Comparative Study of Automatic Language Identification on Ethio-Semitic Languages},
  author   = {Rediat Bekele Asfaw},
  journal  = {Master's Thesis, submitted to Addis Ababa University},
  month    = {June},
  year     = {2018},
  school   = {Addis Ababa University},
  doi      = {http://dx.doi.org/10.13140/RG.2.2.10377.70245},
  tasks    = {Language Identification},
  abstract = {The dominant languages under the family of Ethio-Semitic languages are Amharic, Geez, Guragigna and Tigrigna. From the findings of the language identification studies on European languages, there is a conclusion that most classifiers performance reached the accuracy of 100\%. Local and global studied confirmed that Naïve Bayes Classifier (NBC) classifier does not reached the accuracy level of 100\% in language identification especially on shorter test strings. Comparative Language Identification studies in European languages shows that Cumulative Frequency Addition (CFA) performs close to 100\% accuracies better than the NBC classifier. The purpose of our study is to assess the performance of CFA as compared to NBC on Ethio-Semitic languages, to validate the research findings of CFA and NBC classifiers, and recommend the classifier, language model, evaluation context and the optimal values of N that performs better in language identification. In this research we have employed and experimental study to measure the performance CFA and NBC classifiers. We have developed a training and test corpus from online bibles written in Amharic, Geez, Guragigna and Tigrigna to generate 5 different character-based n-gram language models. We have measured the classifiers performance using under two different evaluation contexts using 10-fold cross validation. F-score is used as an optimal measure of performance for comparing classifiers performances. The classifiers commonly exhibited higher performance when the length of the test phrase grows from a single word to 2, 3 and beyond to reach an F-score measure beyond 99\%. Both classifiers performed similarly under each context corresponding to the language models and n-grams tested. The language model, fixed length character n-grams with location features, exhibited highest performance in F-score for both classifiers under each evaluation contexts on test strings as short as one-word length. N=5 on Fixed length character n-grams with location features language model is the optimal value of N whereas N=2 is the optimal value for the remaining language models on both CFA and NBC classifiers and evaluation contexts. Based on our findings CFA is a classifier that performs better as compared to NBC as it is founded in sound theoretical assumptions and its performance in language identification.}
}

@article{azath-etal-2020-smt,
  title    = {Statistical Machine Translator For English To Tigrigna Translation},
  author   = {Azath and Tsegay Kiros},
  journal  = {International Journal of Scientific & Technology Research},
  year     = {2020},
  pages    = {2095-2099},
  volume   = {9},
  tasks    = {Machine Translation},
  abstract = {Machine Translation is the automatic translation of text from a source language to the target language. The demand for translation has been increasing due to the exchange of information between various regions using different regional languages. English-Tigrigna Statistical Machine Translation, therefore, is required since a lot of documents are written in English. This research study used statistical machine translation approach due to it yields high accuracy and does not need linguistic rules which exploit human effort (knowledge). The language model, Translation model, and decoder are the three basic components in Statistical Machine Translation (SMT). Moses' decoder, Giza++, IRSTLM, and BLEU (Bilingual Evaluation Understudy) are tools that helped to conduct the experiments. 17,338 sentences of bilingual corpus for training, 1000 sentences for test set and 42,284 sentences for language model were used for experiment. The BLEU score produced from the experiment was 23.27\% which would still not enough for applicable applications. As a result, the effect of word factored or segmentation in the translation quality is reduced by increasing the data size of the corpus.}
}

@mastersthesis{bahre-2022-hate,
  title    = {Hate Speech Detection from Facebook Social Media Posts and Comments in Tigrigna language},
  author   = {Weldemariam Bahre},
  journal  = {Master's Thesis, submitted to St. Mary's University},
  month    = {June},
  year     = {2022},
  school   = {The faculty of informatics, St. Mary's University},
  url      = {http://repository.smuc.edu.et/handle/123456789/6929},
  tasks    = {Hate/Abusiveness Detection},
  abstract = {In recent years, hate speech on social media has become a common phenomenon in the Ethiopian online community particularly due to the substantial growth of users. As part of our country language Tigrigna language Facebook users also increased in recent years. In line with this, the hate speech in Tigrigna language is also increased. The reason could be due to, the political instabilities. Hate speech on social media has the potential to quickly disseminate through the online users that could escalate an act of violence and hate crime among peoples. To address this problem, this research proposed hate speech detection using machine learning and text-mining feature extraction techniques to build a detection model. A hate speech data written in Tigrigna language was collected from the Facebook public page and manually labeled into hate and hate-free classes to build binary class datasets. The research employed an experimental approach to determine the best combination of the machine learning algorithm and features extraction for modeling. Support Vector Machine (SVM), Naïve Bayes (NB) and Random Forest (RF)classification algorithms are employed to construct hate speech detection model using the whole dataset with the extracted features based on word unigram, bigram, trigram, as well as combined n-grams and TF*IDF. An experimental result shows that the Naïve Bayes classification algorithm with TF*DF feature extraction were achieved slightly better performance than the SVM and RF models for hate speech detection with 79\% accuracy. In this study we achieved a promising result for designing hate speech detection for Tigrigna language. Since there is no data set available for experimentation, we used limited data for constructing an optimal hate speech detection model using machine learning classification algorithm. Hence, we recommend the need to prepare standard corpus for hate speech detection in local languages, including Tigrigna language.}
}

@inproceedings{bandarkar-etal-2024-belebele,
  title     = {The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants},
  author    = {Bandarkar, Lucas and Liang, Davis and Muller, Benjamin and Artetxe, Mikel and Shukla, Satya Narayan and Husa, Donald and Goyal, Naman and Krishnan, Abhinandan and Zettlemoyer, Luke and Khabsa, Madian},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {August},
  year      = {2024},
  publisher = {Association for Computational Linguistics},
  pages     = {749--775},
  address   = {Bangkok, Thailand},
  doi       = {10.18653/v1/2024.acl-long.44},
  url       = {https://aclanthology.org/2024.acl-long.44/},
  tasks     = {Question Answering},
  abstract  = {We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the Flores-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and find that despite significant cross-lingual transfer in English-centric LLMs, much smaller MLMs pretrained on balanced multilingual data still understand far more languages. We also observe that larger vocabulary size and conscious vocabulary construction correlate with better performance on low-resource languages. Overall, Belebele opens up new avenues for evaluating and analyzing the multilingual capabilities of NLP systems.},
  editor    = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek}
}

@article{berhane-etal-2025-tinc24,
  title     = {{Towards Neural Named Entity Recognition System in Tigrinya with Large-scale Dataset}},
  author    = {Sham K. Berhane and Simon M. Beyene and Yoel G. Teklit and Ibrahim A. Ibrahim and Natnael A. Teklu and Sirak A. Bereketeab and Fitsum Gaim},
  journal   = {Language Resources and Evaluation},
  month     = {Sep},
  year      = {2025},
  publisher = {Springer Nature},
  pages     = {3311--3339},
  volume    = {59},
  doi       = {10.1007/s10579-025-09825-4},
  url       = {https://doi.org/10.1007/s10579-025-09825-4},
  tasks     = {Named Entity Recognition, Part of Speech Tagging},
  abstract  = {The scarcity of annotated datasets remains a significant impediment to Natural Language Processing (NLP) advancement in low-resourced languages. In this work, we introduce a large-scale annotated Tigrinya Named Entity Recognition (NER) Corpus along with state-of-the-art models for Tigrinya NER. Our human-labeled Tigrinya NER Corpus (TiNC24) comprises over 200K words tagged for NER with over 118K of the tokens also annotated with Parts-of-Speech (POS) tags, encompassing eight distinct classes of entities and multiple tagging schemes spanning ten diverse domains. We performed extensive experiments covering several recurrent neural networks and Transformer-based language models, achieving a highest performance of 90.18\% weighted F1-score with the IO tagging scheme. These results are particularly notable given the unique challenges posed by Tigrinya’s distinct grammatical structure and complex word morphology. This work establishes new benchmarks for Tigrinya NLP and provides essential resources for developing NER systems in other related morphologically rich, low-resourced languages. The dataset and models are made publicly available.},
  issn      = {1574-0218},
  number    = {3}
}

@inproceedings{berihu-2020-mt,
  title     = {Enhancing Bi-directional English-Tigrigna Machine Translation Using Hybrid Approach},
  author    = {Zemicheal Berihu and Gebremariam Mesfin and Mulugeta Atsibaha and Tor-Morten and Gr{\o}nli},
  booktitle = {},
  year      = {2020},
  tasks     = {Machine Translation},
  abstract  = {Machine Translation (MT) is an application area of NLP where automatic systems are used to translate text or speech from one language to another while preserving the meaning of the source language. Although there exists a large volume of literature in automatic machine translation of documents in many languages, the translation between English and Tigrigna is less explored. Therefore, we proposed the hybrid approach to address the challenges of applying syntactic reordering rules which align and capture the structural arrangement of words in the source sentence to become more like the target sentences. Two language models were developedone for English and another for Tigrigna and about 12,000 parallel sentences in four domains and 32,000 bilingual dictionaries were collected for our experiment. The parallel collected corpus was split randomly to 10,800 sentences for training set and 1,200 sentences for testing. Moses open source statistical machine translation system has been used for the experiment to train, tune and decode. The parallel corpus was aligned using the Giza++ toolkit and SRILM was used for building the language model. Three main experiments were conducted using statistical approach, hybrid approach and post-processing technique. According to our experimental result showed good translation output as high as 32.64 BLEU points Google translator and the hybrid approach was found most promising for English-Tigrigna bi-directional translation.}
}

@inproceedings{feleke-2017-similarity,
  title     = {The similarity and Mutual Intelligibility between {A}mharic and {T}igrigna Varieties},
  author    = {Feleke, Tekabe Legesse},
  booktitle = {Proceedings of the Fourth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial)},
  month     = {April},
  year      = {2017},
  publisher = {Association for Computational Linguistics},
  pages     = {47--54},
  address   = {Valencia, Spain},
  doi       = {10.18653/v1/W17-1206},
  url       = {https://aclanthology.org/W17-1206},
  tasks     = {Language Identification},
  abstract  = {The present study has examined the similarity and the mutual intelligibility between Amharic and Tigrigna using three tools namely Levenshtein distance, intelligibility test and questionnaires. The study has shown that both Tigrigna varieties have almost equal phonetic and lexical distances from Amharic. The study also indicated that Amharic speakers understand less than 50{\%} of the two varieties. Furthermore, the study showed that Amharic speakers are more positive about the Ethiopian Tigrigna variety than the Eritrean Variety. However, their attitude towards the two varieties does not have an impact on their intelligibility. The Amharic speakers{'} familiarity to the Tigrigna varieties is largely dependent on the genealogical relation between Amharic and the two Tigrigna varieties.}
}

@article{fesseha-etal-2021-text-classification,
  title          = {Text Classification Based on Convolutional Neural Networks and Word Embedding for Low-Resource Languages: Tigrinya},
  author         = {Fesseha, Awet and Xiong, Shengwu and Emiru, Eshete Derb and Diallo, Moussa and Dahou, Abdelghani},
  journal        = {Information},
  year           = {2021},
  volume         = {12},
  doi            = {10.3390/info12020052},
  url            = {https://www.mdpi.com/2078-2489/12/2/52},
  tasks          = {Topic Classification},
  abstract       = {This article studies convolutional neural networks for Tigrinya (also referred to as Tigrigna), which is a family of Semitic languages spoken in Eritrea and northern Ethiopia. Tigrinya is a “low-resource” language and is notable in terms of the absence of comprehensive and free data. Furthermore, it is characterized as one of the most semantically and syntactically complex languages in the world, similar to other Semitic languages. To the best of our knowledge, no previous research has been conducted on the state-of-the-art embedding technique that is shown here. We investigate which word representation methods perform better in terms of learning for single-label text classification problems, which are common when dealing with morphologically rich and complex languages. Manually annotated datasets are used here, where one contains 30,000 Tigrinya news texts from various sources with six categories of “sport”, “agriculture”, “politics”, “religion”, “education”, and “health” and one unannotated corpus that contains more than six million words. In this paper, we explore pretrained word embedding architectures using various convolutional neural networks (CNNs) to predict class labels. We construct a CNN with a continuous bag-of-words (CBOW) method, a CNN with a skip-gram method, and CNNs with and without word2vec and FastText to evaluate Tigrinya news articles. We also compare the CNN results with traditional machine learning models and evaluate the results in terms of the accuracy, precision, recall, and F1 scoring techniques. The CBOW CNN with word2vec achieves the best accuracy with 93.41\%, significantly improving the accuracy for Tigrinya news classification.},
  article-number = {52},
  issn           = {2078-2489},
  number         = {2}
}

@mastersthesis{gaim-2017-morphology-mt,
  title    = {{Applying Morphological Segmentation to Machine Translation of Low-Resourced and Morphologically Complex Languages: The case of Tigrinya}},
  author   = {Fitsum Gaim},
  journal  = {Master's Thesis, submitted to Korea Advanced Institute of Science and Technology ({KAIST})},
  month    = {July},
  year     = {2017},
  school   = {School of Computing, Korea Advanced Institute of Science and Technology ({KAIST})},
  tasks    = {Machine Translation, Morphological Processing},
  abstract = {Machine Translation (MT) has seen substantial advances in recent years, but it remains unexplored and an ongoing challenge for most language pairs. In this thesis, we present the development of a Statistical Machine Translation (SMT) between English and a lesser-known Semitic language, Tigrinya. To the best of our knowledge, this is the first study of machine translation involving the Tigrinya language. Two of the most important factors that affect the performance of SMT systems for a given language pair are: (1) the volume of parallel data available, and (2) the language difference between the pair. In this regard, English and Tigrinya make a particularly difficult pair for the task of SMT. The English language is deeply studied and has a wealth of resources, whereas Tigrinya is much less studied with severely limited computational resources. What is more, the two languages differ markedly in syntax and morphology, particularly in the word structure. Tigrinya is an agglutinative language with a highly derivational and inflectional morphology that proliferates vocabulary and necessitates sub-word translation. Regardless of the salient differences in the making of a word among natural languages, the standard SMT approaches treat surface words as the smallest unit of translation. These techniques work fairly well for languages with simple morphology and relatively small vocabulary such as English. However, they perform suboptimal when languages with rich morphology and huge vocabulary are involved, owing it to poor phrase alignment, data sparsity, and high rate of out-of- vocabulary words. In this empirical study, we build the necessary corpora from scratch and study the effects of both rule-based and unsupervised morphological segmentation of Tigrinya words as remedial measures. Moreover, we augment the system with additional bilingual lexicon to ameliorate the out-of-vocabulary problem. To this end, we have achieved cumulative BLEU scores of 23.3 and 27.14 points for English into Tigrinya, and Tigrinya into English translations, respectively. In the end, the system is published online for public use and the dataset, which comprises 30.6k sentences of parallel corpus and 913k sentences of monolingual corpus, is also made publicly available for researchers.}
}

@misc{gaim-2021-glocr,
  title     = {{GLOCR: GeezLab OCR Dataset}},
  author    = {Fitsum Gaim},
  year      = {2021},
  month     = {April},
  publisher = {Harvard Dataverse},
  version   = {1.0},
  doi       = {10.7910/DVN/RQTSD2},
  url       = {https://doi.org/10.7910/DVN/RQTSD2},
  dataverse = {https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/RQTSD2},
  tasks     = {Optical Character Recognition},
  abstract  = {A Text Recognition (TR) and Optical Character Recognition (OCR) dataset for the Tigrinya language. The dataset contains a total of 710k image-label pairs from multiple data sources. In addition to the characters-only data, the major part of the dataset is a collection of multi-word text images with labels from three categories: News (from Haddas Ertra newspaper), the Bible, and random-trigrams of the 150k most common words in Tigrinya.}
}

@misc{gaim-2024-semantic-search,
  title     = {{Semantic Search Models for Tigrinya}},
  author    = {Fitsum Gaim},
  month     = {January},
  year      = {2024},
  publisher = {Hugging Face Hub},
  doi       = {10.57967/hf/6068},
  url       = {https://huggingface.co/fgaim/tiroberta-bi-encoder},
  tasks     = {Information Retrieval, Language Modeling},
  abstract  = {This work introduces monolingual bi-encoder language models for Tigrinya. The models are based on the TiRoBERTa and TiELECTRA architectures and are trained on Tigrinya question-answering and information retrieval datasets. The models are designed to support semantic search tasks, such as information retrieval, text representation, and question answering.}
}

@inproceedings{gaim-etal-2021-tiplms,
  title     = {Monolingual Pre-trained Language Models for Tigrinya},
  author    = {Fitsum Gaim and Wonsuk Yang and Jong C. Park},
  booktitle = {5th Widening NLP (WiNLP2021) workshop, co-located with the 2021 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  year      = {2021},
  url       = {http://www.winlp.org/wp-content/uploads/2021/11/winlp2021_62_Paper.pdf},
  tasks     = {Language Modeling, Part of Speech Tagging, Sentiment Analysis},
  abstract  = {Pre-trained language models (PLMs) are driving much of the recent progress in natural language processing. However, due to the resource-intensive nature of the models, underrepresented languages without sizable curated data have not seen significant progress. Multilingual PLMs have been introduced with the potential to generalize across many languages, but their performance trails compared to their monolingual counterparts and depends on the characteristics of the target language. In the case of the Tigrinya language, recent studies report a sub-optimal performance when applying the current multilingual models. This may be due to its orthography and unique linguistic characteristics, especially when compared to the Indo-European and other typologically distant languages that were used to train the models. In this work, we pre-train three monolingual PLMs for Tigrinya on a newly compiled corpus, and we compare the models with their multilingual counterparts on two downstream tasks, part-of-speech tagging and sentiment analysis, achieving significantly better results and establishing the state-of-the-art. We make the data and trained models publicly available.}
}

@article{gaim-etal-2021-tlmd,
  title     = {TLMD: Tigrinya Language Modeling Dataset},
  author    = {Fitsum Gaim and Wonsuk Yang and Jong C. Park},
  journal   = {Zenodo},
  month     = {July},
  year      = {2021},
  publisher = {Zenodo},
  version   = {1.0.0},
  doi       = {10.5281/zenodo.5139094},
  url       = {https://doi.org/10.5281/zenodo.5139094},
  tasks     = {Language Modeling},
  abstract  = {A monolingual dataset built for Tigrinya language modeling. To the best of our knowledge, this is the largest dataset for Tigrinya of its kind. The data was collected from various sources across the web including news, blogs, and books. The largest portion of the data, ~75\%, comes from over 2150 issues of the Haddas Ertra newspaper and other magazines published by www.shabait.com. Data Statistics: Total size: ~0.5GB; Around 40 million tokens; Over 2 million lines; 367 unique characters; Train split: 98\%, 1.97 million lines; Validation split: 2\%, 43k lines. We have done a light-weight cleanup of the data: - Removal of Tigrinya text with legacy and non-standard encoding systems. - Normalization of punctuation and special characters. - Removal of redundant white spaces and empty lines. - Rejoining or fixing broken sentences when possible. - Removal of foreign words. We avoid applying any form of tokenization, extensive cleanup, and preprocessing operations in order not to take away potentially useful information, those decisions are left to the use-case researchers or developers.}
}

@misc{gaim-etal-2022-analogy-test,
  title     = {{Tigrinya Analogy Test for evaluating Word Embeddings}},
  author    = {Fitsum Gaim and Jong C. Park},
  month     = {May},
  year      = {2022},
  publisher = {Zenodo},
  version   = {1.0.0},
  doi       = {10.5281/zenodo.7089051},
  url       = {https://doi.org/10.5281/zenodo.7089051},
  github    = {https://github.com/fgaim/tigrinya-analogy-test},
  tasks     = {Word Embedding},
  abstract  = {This is a Tigrinya version of the Google Analogy Test set, which is used to evaluate English word-embedding models. The analogy test is a well-established strategy to empirically evaluate the quality of word-embedding models. More information about the English task can be found at the ACL Wiki. The data was first machine translated then manually verified by a native speaker to reduce errors. Some aspects of the original analogy test is focused on English and may not transfer well to other languages, such as those related to grammar or morphology. Therefore, when adapting the task we have discarded examples that were discovered as irrelevant in Tigrinya. Finally, there are a total of 18,465 entries in the Tigrinya Analogy Test set, while the source English data has 19,544 entries.}
}

@inproceedings{gaim-etal-2022-geezswitch,
  title     = {{G}eez{S}witch: Language Identification in Typologically Related Low-resourced {E}ast {A}frican Languages},
  author    = {Fitsum Gaim and Wonsuk Yang and Jong C. Park},
  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
  month     = {June},
  year      = {2022},
  publisher = {European Language Resources Association},
  pages     = {6578--6584},
  address   = {Marseille, France},
  url       = {https://aclanthology.org/2022.lrec-1.707},
  tasks     = {Language Identification},
  abstract  = {Language identification is one of the fundamental tasks in natural language processing that is a prerequisite to data processing and numerous applications. Low-resourced languages with similar typologies are generally confused with each other in real-world applications such as machine translation, affecting the user{'}s experience. In this work, we present a language identification dataset for five typologically and phylogenetically related low-resourced East African languages that use the Ge{'}ez script as a writing system; namely Amharic, Blin, Ge{'}ez, Tigre, and Tigrinya. The dataset is built automatically from selected data sources, but we also performed a manual evaluation to assess its quality. Our approach to constructing the dataset is cost-effective and applicable to other low-resource languages. We integrated the dataset into an existing language-identification tool and also fine-tuned several Transformer based language models, achieving very strong results in all cases. While the task of language identification is easy for the informed person, such datasets can make a difference in real-world deployments and also serve as part of a benchmark for language understanding in the target languages. The data and models are made available at https://github.com/fgaim/geezswitch.}
}

@inproceedings{gaim-etal-2023-question,
  title     = {Question-Answering in a Low-resourced Language: Benchmark Dataset and Models for {T}igrinya},
  author    = {Fitsum Gaim and Wonsuk Yang and Hancheol Park and Jong C. Park},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2023},
  publisher = {Association for Computational Linguistics},
  pages     = {11857--11870},
  address   = {Toronto, Canada},
  url       = {https://aclanthology.org/2023.acl-long.661},
  tasks     = {Question Answering},
  abstract  = {Question-Answering (QA) has seen significant advances recently, achieving near human-level performance over some benchmarks. However, these advances focus on high-resourced languages such as English, while the task remains unexplored for most other languages, mainly due to the lack of annotated datasets. This work presents a native QA dataset for an East African language, Tigrinya. The dataset contains 10.6K question-answer pairs spanning 572 paragraphs extracted from 290 news articles on various topics. The dataset construction method is discussed, which is applicable to constructing similar resources for related languages. We present comprehensive experiments and analyses of several resource-efficient approaches to QA, including monolingual, cross-lingual, and multilingual setups, along with comparisons against machine-translated silver data. Our strong baseline models reach 76{\%} in the F1 score, while the estimated human performance is 92{\%}, indicating that the benchmark presents a good challenge for future work. We make the dataset, models, and leaderboard publicly available.}
}

@inproceedings{gaim-etal-2025-tiald,
  title     = {{A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings}},
  author    = {Fitsum Gaim and Hoyun Song and Huije Lee and Changgeon Ko and Eui Jun Hwang and Jong C. Park},
  booktitle = {The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS): Datasets and Benchmarks Track},
  month     = {December},
  year      = {2025},
  address   = {NeurIPS 2025, San Diego, CA, USA},
  url       = {https://openreview.net/forum?id=AB2uK8FhQe},
  tasks     = {Hate/Abusiveness Detection, Sentiment Analysis, Topic Classification},
  abstract  = {Content moderation research has recently made significant advances, but remains limited in serving the majority of the world's languages due to the lack of resources, leaving millions of vulnerable users to online hostility. This work presents a large-scale human-annotated multi-task benchmark dataset for abusive language detection in Tigrinya social media with joint annotations for three tasks: abusiveness, sentiment, and topic classification. The dataset comprises 13,717 YouTube comments annotated by nine native speakers, collected from 7,373 videos with a total of over 1.2 billion views across 51 channels. We developed an iterative term clustering approach for effective data selection. Recognizing that around 64% of Tigrinya social media content uses Romanized transliterations rather than native Ge'ez script, our dataset accommodates both writing systems to reflect actual language use. We establish strong baselines across the tasks in the benchmark, while leaving significant challenges for future contributions. Our experiments demonstrate that small fine-tuned models outperform prompted frontier large language models (LLMs) in the low-resource setting, achieving 86.67% F1 in abusiveness detection (7+ points over best LLM), and maintain stronger performance in all other tasks. The benchmark is made public to promote research on online safety.}
}

@inproceedings{gasser-etal-2011-hornmorpho,
  title     = {HornMorpho: a system for morphological processing of Amharic, Oromo, and Tigrinya},
  author    = {Gasser, Michael},
  booktitle = {Conference on Human Language Technology for Development, Alexandria, Egypt},
  year      = {2011},
  tasks     = {Morphological Processing},
  abstract  = {Despite its linguistic complexity, the Horn of Africa region includes several major languages with more than 5 million speakers, some crossing the borders of multiple countries. All of these languages have official status in regions or nations and are crucial for development; yet computational resources for the languages remain limited or non-existent. Since these languages are complex morphologically, software for morphological analysis and generation is a necessary first step toward nearly all other applications. This paper describes a resource for morphological analysis and generation for three of the most important languages in the Horn of Africa, Amharic, Tigrinya, and Oromo.}
}

@article{gebremeskel-etal-2023-hybrid-morphological-analyzer,
  title    = {Unlock Tigrigna NLP: Design and Development of Morphological Analyzer for Tigrigna Verbs Using Hybrid Approach},
  author   = {Hagos Gebremedhin Gebremeskel and Feng Chong and Huang Heyan},
  journal  = {SSRN Electronic Journal},
  year     = {2023},
  url      = {https://api.semanticscholar.org/CorpusID:265633133},
  tasks    = {Morphological Processing},
  abstract = {Morphological analyzer is the basis for various high-level NLP applications such as information retrieval, spell checking, grammar checking, machine translation, speech recognition, POS tagging and automatic sentence construction. This paper is carefully designed for the analysis of Tigrigna verbs morphological analyzer using the hybrid of memory learning and rule based approaches. The experiment have conducted using Python 3 where TiMBL algorithms IB2 and TRIBL2, and Finite State Transducer rules are used. The performance of the system has been evaluated using 10 fold cross-validation technique. Testing was conducted using optimized parameter settings for regular verbs and linguistic rules of the Tigrigna language allomorph and phonology for the irregular verbs. The accuracy of the memory based approach with optimized parameters of TiMBL algorithm IB2 and TRIBL2 was 93.24\% and 92.31\%, respectively. Finally, the hybrid approach had an actual performance of 95.6\% using linguistic rules for handling irregular and copula verbs.}
}

@inproceedings{gedamu-2023-tigrinya-dialect,
  title     = {Tigrinya Dialect Identification},
  author    = {Asfaw Gedamu and Asmelash Teka Hadgu},
  booktitle = {AfricaNLP workshop at ICLR2023},
  year      = {2023},
  url       = {https://openreview.net/pdf?id=kiG8qiUFm2u},
  tasks     = {Language Identification},
  abstract  = {Dialect Identification is an important topic of research in Natural Language Processing (NLP) as it has broad implications in many real-world applications such as machine translation, speech recognition and chatbots to name a few. In this work, we investigate Tigrinya dialect identification using machine learning techniques. To that end, we have identified three Tigrinya dialects, namely: Z, L and D. Then we systematically collected datasets for each dialect. Finally, we perform experiments using classical machine learning and deep learning methods to quantify effectiveness of current methods on the problem of Tigrinya dialect identification. The highest overall accuracy of 92.98\% was achieved using character-level Convolutional Neural Networks (CNNs).}
}

@inproceedings{ghebregiorgis-etal-2024-tigrinya-asr,
  title     = {{Tigrinya End-to-End Speech Recognition}: {A} Hybrid Connectionist Temporal Classification-Attention Approach},
  author    = {Ghebregiorgis, Bereket Desbele and Tekle, Yonatan Yosef and Kidane, Mebrahtu Fisshaye and Keleta, Mussie Kaleab and Ghebraeb, Rutta Fissehatsion and Gebretatios, Daniel Tesfai},
  booktitle = {Pan-African Conference on Artificial Intelligence},
  year      = {2024},
  publisher = {Springer Nature Switzerland},
  pages     = {221--236},
  address   = {Cham},
  doi       = {10.1007/978-3-031-57624-9_12},
  isbn      = {978-3-031-57624-9},
  url       = {https://link.springer.com/chapter/10.1007/978-3-031-57624-9_12},
  tasks     = {Speech Recognition},
  abstract  = {The latest improvements in end-to-end Automatic Speech Recognition (ASR) systems have achieved outstanding results and have thus enabled the creation of state-of-the-art models for well-resourced languages. However, most languages, such as Tigrinya, are under-resourced, discouraging field efforts. Tigrinya is a Semitic language with over nine million speakers. This paper presents the first hybrid Connectionist Temporal Classification (CTC) with an attention-based end-to-end speaker-independent ASR model for Tigrinya. This initiative constructed new text and speech corpora encompassing multiple domains and thorough pre-processing, which amounted to about 170,000 phrases and sentences of text and 30h of speech corpus. Data augmentation was applied to generate synthetic data for better generalization capability. A Recurrent Neural Network Language Model (RNN-LM) was also used for post-processing to complement the model to achieve even better results. Multiple experiments were conducted with different settings and parameters. Whilst keeping the data size/split constant and employing various combinations of data augmentation techniques along with varying LM's vocabulary size showed improved performances, increasing the vocabulary size from 5k to 20k resulted in minute decoding improvement. Our best model exhibited a Character Error Rate (CER) of 14.28{\%} and a Word Error Rate (WER) of 36.01{\%}, which is significant considering this end-to-end approach is the first of its kind for the under-resourced Tigrinya language.},
  editor    = {Debelee, Taye Girma and Ibenthal, Achim and Schwenker, Friedhelm and Megersa Ayano, Yehualashet}
}

@inproceedings{hadgu-etal-2022-lesan,
  title        = {Lesan--machine translation for low resource languages},
  author       = {Hadgu, Asmelash Teka and Aregawi, Abel and Beaudoin, Adam},
  booktitle    = {NeurIPS 2021 Competitions and Demonstrations Track},
  year         = {2022},
  organization = {PMLR},
  pages        = {297--301},
  url          = {https://proceedings.mlr.press/v176/hadgu22a/hadgu22a.pdf},
  tasks        = {Machine Translation},
  abstract     = {Millions of people around the world can not access content on the Web because most of the content is not readily available in their language. Machine translation (MT) systems have the potential to change this for many languages. Current MT systems provide very accurate results for high resource language pairs, e.g., German and English. However, for many low resource languages, MT is still under active research. The key challenge is lack of datasets to build these systems. We present Lesan (https://lesan.ai/), an MT system for low resource languages. Our pipeline solves the key bottleneck to low resource MT by leveraging online and offline sources, a custom Optical Character Recognition (OCR) system for Ethiopic and an automatic alignment module. The final step in the pipeline is a sequence to sequence model that takes parallel corpus as input and gives us a translation model. Lesan{’}s translation model is based on the Transformer architecture. After constructing a base model, back translation is used to leverage monolingual corpora. Currently Lesan supports translation to and from Tigrinya, Amharic and English. We perform extensive human evaluation and show that Lesan outperforms state-of-the-art systems such as Google Translate and Microsoft Translator across all six pairs. Lesan is freely available and has served more than 10 million translations so far. At the moment, there are only 217 Tigrinya and 15,009 Amharic Wikipedia articles. We believe that Lesan will contribute towards democratizing access to the Web through MT for millions of people.}
}

@inproceedings{hailu-etal-2024-tigrinya-ocr,
  title     = {Tigrinya OCR: Applying CRNN for Text Recognition},
  author    = {Hailu, Aaron Afewerki and Hayleslassie, Abiel Tesfamichael and Gebresilasie, Danait Weldu and Haile, Robel Estifanos and Ghebremedhin, Tesfana Tekeste and Tedla, Yemane Keleta},
  booktitle = {Neural Information Processing},
  year      = {2024},
  publisher = {Springer Nature Singapore},
  pages     = {456--467},
  address   = {Singapore},
  isbn      = {978-981-99-8184-7},
  tasks     = {Optical Character Recognition},
  editor    = {Luo, Biao and Cheng, Long and Wu, Zheng-Guang and Li, Hongyi and Li, Chaojie}
}

@article{isayas-2021-nmt,
  title    = {A First Look into Neural Machine Translation for Tigrinya},
  author   = {Adhanom, Isayas Berhe},
  journal  = {arXiv},
  year     = {2021},
  tasks    = {Machine Translation},
  abstract = {This paper presents a study that investigates the use of current neural machine translation (NMT) techniques for Tigrinya - a low-resourced Semitic language. The proposed method achieves state-of-the-art performance on the task of English-Tigrinya translation using the JW300 En-Ti dataset. The study uses the transformer architecture to achieve translation performance that improves over previous techniques. Additionally, an evaluation dataset and a monolingualTigrinya corpus are built as part of the study and are made available for public use. We also evaluate the effectiveness of back-translation for NMT in Tigrinyaby adding more than a million sentences, generated by back-translation from our monolingual corpus, to our small-sized parallel corpus. By releasing these resources in addition to the code and the results produced in this study, this study aims to provide a benchmark for other researchers in Tigrinya machine translation to compare and build upon.}
}

@article{keletay-worku-2020-tts,
  title     = {Developing Concatenative Based Text to Speech Synthesizer for {Tigrigna} Language},
  author    = {Keletay, Mezgebe Araya and Worku, Hussien Seid},
  journal   = {Internet of Things and Cloud Computing},
  year      = {2020},
  publisher = {Science Publishing Group},
  pages     = {24--30},
  volume    = {8},
  doi       = {10.11648/j.iotcc.20200802.12},
  url       = {https://doi.org/10.11648/j.iotcc.20200802.12},
  tasks     = {Text-to-Speech Synthesis},
  abstract  = {A Text-To-Speech (TTS) synthesizer is a computer-based system able to read any text and convert it into speech that resembles as closely as possible a native speaker of the language. This thesis describes the first Text-to-Speech (TTS) system for the Tigrigna language, using speech synthesis architecture in MATLAB. The TTS system is working based on concatenative synthesis and applying LPC technique. The performance of the system is measured and the quality of synthesized speech is assessed in terms of intelligibility and naturalness. The result of the synthesizer is evaluated in two ways, in word level and sentences level. The test results indicate in the word level is evaluated by NeoSpeech tool online and most of the words are recognizable. The overall performance of the system in the word level which is evaluated by NeoSpeech tool is found to be 78\%. When it comes to the intelligibility and naturalness of the synthesized speech in the sentence level, it is measured in MOS scale and the overall intelligibility and naturalness of the system is found to be 3.28 and 3.27 respectively. The values of performance, intelligibility and naturalness are encouraging and show that diphone speech units are good candidates to develop fully functional speech synthesizer. But there are areas that can be improved. Inclusion of text analyzer to pronounce zonal dialects of the language and prosody generator are some of the things that need further investigation.},
  number    = {2}
}

@article{kidane-etal-2021-augmentation,
  title    = {An Exploration of Data Augmentation Techniques for Improving English to Tigrinya Translation},
  author   = {Lidia Kidane and Sachin Kumar and Yulia Tsvetkov},
  journal  = {ArXiv},
  year     = {2021},
  volume   = {abs/2103.16789},
  tasks    = {Machine Translation},
  abstract = {It has been shown that the performance of neural machine translation (NMT) drops starkly in low-resource conditions, often requiring large amounts of auxiliary data to achieve competitive results. An effective method of generating auxiliary data is back-translation of target language sentences. In this work, we present a case study of Tigrinya where we investigate several back-translation methods to generate synthetic source sentences. We find that in low-resource conditions, back-translation by pivoting through a higher-resource language related to the target language, proves most effective resulting in substantial improvements over baselines.}
}

@inproceedings{littell-etal-2018-parser,
  title     = {Parser combinators for {T}igrinya and {O}romo morphology},
  author    = {Littell, Patrick  and McCoy, Tom  and Han, Na-Rae  and Rijhwani, Shruti  and Sheikh, Zaid  and Mortensen, David  and Mitamura, Teruko  and Levin, Lori},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},
  month     = {May},
  year      = {2018},
  publisher = {European Language Resources Association (ELRA)},
  address   = {Miyazaki, Japan},
  url       = {https://aclanthology.org/L18-1611},
  tasks     = {Morphological Processing},
  abstract  = {We present rule-based morphological parsers in the Tigrinya and Oromo languages, based on a parser-combinator rather than finite-state paradigm. This paradigm allows rapid development and ease of integration with other systems, although at the cost of non-optimal theoretical efficiency. These parsers produce multiple output representations simultaneously, including lemmatization, morphological segmentation, and an English word-for-word gloss, and we evaluate these representations as input for entity detection and linking and humanitarian need detection.}
}

@article{meta-etal-2024-nllb,
  title     = {Scaling neural machine translation to 200 languages},
  author    = {{NLLB Team}, Costa-jussà, Marta R. and Cross, James and Çelebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and Sun, Anna and Wang, Skyler and Wenzek, Guillaume and Youngblood, Al and Akula, Bapi and Barrault, Loic and Gonzalez, Gabriel Mejia and Hansanti, Prangthip and Hoffman, John and Jarrett, Semarley and Sadagopan, Kaushik Ram and Rowe, Dirk and Spruit, Shannon and Tran, Chau and Andrews, Pierre and Ayan, Necip Fazil and Bhosale, Shruti and Edunov, Sergey and Fan, Angela and Gao, Cynthia and Goswami, Vedanuj and Guzmán, Francisco and Koehn, Philipp and Mourachko, Alexandre and Ropers, Christophe and Saleem, Safiyyah and Schwenk, Holger and Wang, Jeff},
  journal   = {Nature},
  month     = {June},
  year      = {2024},
  publisher = {Nature Publishing Group},
  pages     = {841--846},
  volume    = {630},
  doi       = {10.1038/s41586-024-07335-x},
  url       = {https://doi.org/10.1038/s41586-024-07335-x},
  tasks     = {Machine Translation},
  abstract  = {The development of neural techniques has opened up new avenues for research in machine translation. Today, neural machine translation (NMT) systems can leverage highly multilingual capacities and even perform zero-shot translation, delivering promising results in terms of language coverage and quality. However, scaling quality NMT requires large volumes of parallel bilingual data, which are not equally available for the 7,000+ languages in the world1. Focusing on improving the translation qualities of a relatively small group of high-resource languages comes at the expense of directing research attention to low-resource languages, exacerbating digital inequities in the long run. To break this pattern, here we introduce No Language Left Behind—a single massively multilingual model that leverages transfer learning across languages. We developed a conditional computational model based on the Sparsely Gated Mixture of Experts architecture2,3,4,5,6,7, which we trained on data obtained with new mining techniques tailored for low-resource languages. Furthermore, we devised multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. We evaluated the performance of our model over 40,000 translation directions using tools created specifically for this purpose—an automatic benchmark (FLORES-200), a human evaluation metric (XSTS) and a toxicity detector that covers every language in our model. Compared with the previous state-of-the-art models, our model achieves an average of 44% improvement in translation quality as measured by BLEU. By demonstrating how to scale NMT to 200 languages and making all contributions in this effort freely available for non-commercial use, our work lays important groundwork for the development of a universal translation system.},
  issn      = {1476-4687},
  number    = {8018}
}

@inproceedings{mihreteab-etal-2025-end-to-end-tts,
  title     = {{Towards End-to-End Speech Synthesis for Tigrinya Language}},
  author    = {Mihreteab, M. and Kidane, S. and Fisshaye, M. and Teklemariam, F.},
  booktitle = {Speech and Language Technologies for Low-Resource Languages},
  month     = {Nov},
  year      = {2025},
  publisher = {Springer Nature Switzerland},
  pages     = {21--35},
  address   = {Cham},
  isbn      = {978-3-032-05855-3},
  url       = {https://link.springer.com/chapter/10.1007/978-3-032-05855-3_2},
  tasks     = {Text-to-Speech Synthesis},
  abstract  = {The construction of text-to-speech (TTS) synthesis systems dates as far back as the industrial revolution, and is one of the most important natural language processing (NLP) tasks on the human-computer interaction spectrum. Numerous speech synthesis systems emerged within the past two decades; however, little has been done to extensively research and build such systems for low-resourced languages like Tigrinya. This paper presents a new large-scale speech corpus for Tigrinya, with 17.38 h of transcribed speech (11,639 utterances) of a single speaker. The dataset was trained on the Tacotron model for 600K steps, in which regardless of the training steps' cap, generated fairly intelligible speech but with degraded natural prosody. The predicted speech scored an overall mean opinion score (MOS) of 3.41 when evaluated by experienced sound and audio signal processing experts, which is remarkable considering the unique intonational challenges present in Tigrinya.},
  editor    = {Chakravarthi, Bharathi Raja and B, Bharathi and Rajiakodi, Saranya and Garc{\'i}a Cumbreras, Miguel {\'A}ngel and Jim{\'e}nez Zafra, Salud Mar{\'i}a and Kov{\'a}cs, Gy{\"o}rgy and Eger, Steffen and Wahyu Pamungkas, Endang and Dobrovoljc, Kaja}
}

@inproceedings{muhammad-etal-2025-afrihate,
  title     = {{A}fri{H}ate: A Multilingual Collection of Hate Speech and Abusive Language Datasets for {A}frican Languages},
  author    = {Muhammad, Shamsuddeen Hassan and Abdulmumin, Idris and Ayele, Abinew Ali and Adelani, David Ifeoluwa and Ahmad, Ibrahim Said and Aliyu, Saminu Mohammad and R{"o}ttger, Paul and Oppong, Abigail and Bukula, Andiswa and Chukwuneke, Chiamaka Ijeoma and Jibril, Ebrahim Chekol and Ismail, Elyas Abdi and Alemneh, Esubalew and Gebremichael, Hagos Tesfahun and Aliyu, Lukman Jibril and Beloucif, Meriem and Hourrane, Oumaima and Mabuya, Rooweither and Osei, Salomey and Rutunda, Samuel and Belay, Tadesse Destaw and Guge, Tadesse Kebede and Asfaw, Tesfa Tegegne and Wanzare, Lilian Diana Awuor and Onyango, Nelson Odhiambo and Yimam, Seid Muhie and Ousidhoum, Nedjma},
  booktitle = {Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  month     = {Apr},
  year      = {2025},
  publisher = {Association for Computational Linguistics},
  pages     = {1854--1871},
  address   = {Albuquerque, New Mexico},
  doi       = {10.18653/v1/2025.naacl-long.92},
  isbn      = {979-8-89176-189-6},
  url       = {https://aclanthology.org/2025.naacl-long.92/},
  tasks     = {Hate/Abusiveness Detection},
  abstract  = {Hate speech and abusive language are global phenomena that need socio-cultural background knowledge to be understood, identified, and moderated. However, in many regions of the Global South, there have been several documented occurrences of (1) absence of moderation and (2) censorship due to the reliance on keyword spotting out of context. Further, high-profile individuals have frequently been at the center of the moderation process, while large and targeted hate speech campaigns against minorities have been overlooked.These limitations are mainly due to the lack of high-quality data in the local languages and the failure to include local communities in the collection, annotation, and moderation processes. To address this issue, we present AfriHate: a multilingual collection of hate speech and abusive language datasets in 15 African languages. Each instance in AfriHate is a tweet annotated by native speakers familiar with the regional culture. We report the challenges related to the construction of the datasets and present various classification baseline results with and without using LLMs. We find that model performance highly depends on the language and that multilingual models can help boost performance in low-resource settings.},
  editor    = {Chiruzzo, Luis and Ritter, Alan and Wang, Lu}
}

@inproceedings{ogueji-etal-2021-small,
  title     = {Small Data? No Problem! Exploring the Viability of Pretrained Multilingual Language Models for Low-resourced Languages},
  author    = {Ogueji, Kelechi and Zhu, Yuxin and Lin, Jimmy},
  booktitle = {Proceedings of the 1st Workshop on Multilingual Representation Learning},
  month     = {November},
  year      = {2021},
  publisher = {Association for Computational Linguistics},
  pages     = {116--126},
  address   = {Punta Cana, Dominican Republic},
  doi       = {10.18653/v1/2021.mrl-1.11},
  url       = {https://aclanthology.org/2021.mrl-1.11/},
  tasks     = {Language Modeling},
  abstract  = {Pretrained multilingual language models have been shown to work well on many languages for a variety of downstream NLP tasks. However, these models are known to require a lot of training data. This consequently leaves out a huge percentage of the world’s languages as they are under-resourced. Furthermore, a major motivation behind these models is that lower-resource languages benefit from joint training with higher-resource languages. In this work, we challenge this assumption and present the first attempt at training a multilingual language model on only low-resource languages. We show that it is possible to train competitive multilingual language models on less than 1 GB of text. Our model, named AfriBERTa, covers 11 African languages, including the first language model for 4 of these languages. Evaluations on named entity recognition and text classification spanning 10 languages show that our model outperforms mBERT and XLM-Rin several languages and is very competitive overall. Results suggest that our “small data” approach based on similar languages may sometimes work better than joint training on large datasets with high-resource languages. Code, data and models are released at https://github.com/keleog/afriberta.},
  editor    = {Ataman, Duygu and Birch, Alexandra and Conneau, Alexis and Firat, Orhan and Ruder, Sebastian and Sahin, Gozde Gul}
}

@inproceedings{oktem-etal-2020-mt-transfer,
  title     = {Tigrinya Neural Machine Translation with Transfer Learning for Humanitarian Response},
  author    = {Alp {\"{O}}ktem and Mirko Plitt and Grace Tang},
  booktitle = {1st AfricaNLP Workshop Proceedings, AfricaNLP@ICLR 2020, Virtual Conference},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.11523},
  tasks     = {Machine Translation},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-11523.bib},
  editor    = {Kathleen Siminyu and Laura Martinus and Vukosi Marivate},
  timestamp = {Wed, 06 Sep 2023 09:56:29 +0200}
}

@inproceedings{osman-mikami-2012-stemming,
  title     = {Stemming {T}igrinya Words for Information Retrieval},
  author    = {Osman, Omer and Mikami, Yoshiki},
  booktitle = {Proceedings of {COLING} 2012: Demonstration Papers},
  month     = {December},
  year      = {2012},
  publisher = {The COLING 2012 Organizing Committee},
  pages     = {345--352},
  address   = {Mumbai, India},
  url       = {https://aclanthology.org/C12-3043},
  tasks     = {Morphological Processing, Information Retrieval},
  abstract  = {The increasing penetration of internet into less developed countries has resulted in the increase in the number of digital documents written in many minor languages. However, many of these languages have limited resources in terms of data, language resources and computational tools. Stemming is the reduction of inflected word forms into common basic form. It is an important analysis process in information retrieval and many natural language processing applications. In highly inflected languages such as Tigrinya, stemming is not always straightforward task. In this paper we present the development of stemmer for Tigrinya words to facilitate the information retrieval. We used a hybrid approach for stemming that combines rule based stemming which removes affixes in successively applied steps and dictionary based stemming which reduces stemming errors by verifying the resulting stem based on word distance measures. The stemmer was evaluated using two sets of Tigrinya words. The results show that it achieved an average accuracy of 89.3\%.}
}

@misc{pratap-etal-2023-mms-tts-tir,
  title         = {{Scaling Speech Technology to 1,000+ Languages: Massively Multilingual Speech (MMS) - Tigrigna Text-to-Speech}},
  author        = {Vineel Pratap and Andros Tjandra and Bowen Shi and Paden Tomasello and Arun Babu and Sayani Kundu and Ali Elkahky and Zhaoheng Ni and Apoorv Vyas and Maryam Fazel-Zarandi and Alexei Baevski and Yossi Adi and Xiaohui Zhang and Wei-Ning Hsu and Alexis Conneau and Michael Auli},
  year          = {2023},
  primaryclass  = {cs.CL},
  publisher     = {Meta AI / Hugging Face},
  eprint        = {2305.13516},
  archiveprefix = {arXiv},
  url           = {https://huggingface.co/facebook/mms-tts-tir},
  tasks         = {Text-to-Speech Synthesis},
  abstract      = {Expanding the language coverage of speech technology has the potential to improve access to information for many more people. However, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000 languages spoken around the world. The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task. The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models for the same number of languages, as well as a language identification model for 4,017 languages. Experiments show that our multilingual speech recognition model more than halves the word error rate of Whisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data.},
  note          = {Part of the Massively Multilingual Speech project},
  paper_url     = {https://arxiv.org/abs/2305.13516}
}

@inproceedings{sewunetie-etal-2024-gender-bias,
  title     = {Gender Bias Evaluation in Machine Translation for {A}mharic, {T}igrigna, and Afaan Oromoo},
  author    = {Sewunetie, Walelign  and Tonja, Atnafu  and Belay, Tadesse  and Nigatu, Hellina Hailu  and Gebremeskel, Gashaw  and Mossie, Zewdie  and Seid, Hussien  and Yimam, Seid},
  booktitle = {Proceedings of the 2nd International Workshop on Gender-Inclusive Translation Technologies},
  month     = {June},
  year      = {2024},
  publisher = {European Association for Machine Translation (EAMT)},
  pages     = {1--11},
  address   = {Sheffield, United Kingdom},
  url       = {https://aclanthology.org/2024.gitt-1.1/},
  tasks     = {Machine Translation},
  abstract  = {While Machine Translation (MT) research has progressed over the years, translation systems still suffer from biases, including gender bias. While an active line of research studies the existence and mitigation strategies of gender bias in machine translation systems, there is limited research exploring this phenomenon for low-resource languages. The limited availability of linguistic and computational resources confounded with the lack of benchmark datasets makes studying bias for low-resourced languages that much more difficult. In this paper, we construct benchmark datasets to evaluate gender bias in machine translation for three low-resource languages: Afaan Oromoo (Orm), Amharic (Amh), and Tigrinya (Tir). Building on prior work, we collected 2400 gender-balanced sentences parallelly translated into the three languages. From human evaluations of the dataset we collected, we found that about 93{\%} of Afaan Oromoo, 80{\%} of Tigrinya, and 72{\%} of Amharic sentences exhibited gender bias. In addition to providing benchmarks for improving gender bias mitigation research in the three languages, we hope the careful documentation of our work will help other low-resourced language researchers extend our approach to their languages.},
  editor    = {Savoldi, Beatrice  and Hackenbuchner, Jani{\c{c}}a  and Bentivogli, Luisa  and Daems, Joke  and Vanmassenhove, Eva  and Bastings, Jasmijn}
}

@inproceedings{singh-etal-2024-aya,
  title     = {Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning},
  author    = {Singh, Shivalika and Vargus, Freddie and D{'}souza, Daniel and Karlsson, B{\"o}rje and Mahendiran, Abinaya and Ko, Wei-Yin and Shandilya, Herumb and Patel, Jay and Mataciunas, Deividas and O{'}Mahony, Laura and Zhang, Mike and Hettiarachchi, Ramith and Wilson, Joseph and Machado, Marina and Moura, Luisa and Krzemi{\'n}ski, Dominik and Fadaei, Hakimeh and Ergun, Irem and Okoh, Ifeoma and Alaagib, Aisha and Mudannayake, Oshan and Alyafeai, Zaid and Chien, Vu and Ruder, Sebastian and Guthikonda, Surya and Alghamdi, Emad and Gehrmann, Sebastian and Muennighoff, Niklas and Bartolo, Max and Kreutzer, Julia and {\"U}st{\"u}n, Ahmet and Fadaee, Marzieh and Hooker, Sara},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {Aug},
  year      = {2024},
  publisher = {Association for Computational Linguistics},
  pages     = {11521--11567},
  address   = {Bangkok, Thailand},
  doi       = {10.18653/v1/2024.acl-long.620},
  url       = {https://aclanthology.org/2024.acl-long.620/},
  tasks     = {Language Modeling},
  abstract  = {Datasets are foundational to many breakthroughs in modern artificial intelligence. Many recent achievements in the space of natural language processing (NLP) can be attributed to the fine-tuning of pre-trained models on a diverse set of tasks that enables a large language model (LLM) to respond to instructions. Instruction fine-tuning (IFT) requires specifically constructed and annotated datasets. However, existing datasets are almost all in the English language. In this work, our primary goal is to bridge the language gap by building a human-curated instruction-following dataset spanning 65 languages. We worked with fluent speakers of languages from around the world to collect natural instances of instructions and completions. Furthermore, we create the most extensive multilingual collection to date, comprising 513 million instances through templating and augmenting existing datasets across 114 languages. In total, we contribute three key resources: we develop and open-source the Aya Dataset, the Aya Collection, and the Aya Evaluation Suite. The Aya initiative also serves as a valuable case study in participatory research, involving collaborators from 119 countries. We see this as an important framework for future research collaborations that aim to bridge gaps in resources.},
  editor    = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek}
}

@article{tedla-etal-2016-pos-tagging,
  title    = {Tigrinya Part-of-Speech Tagging with Morphological Patterns and the New Nagaoka Tigrinya Corpus},
  author   = {Yemane Tedla and Kazuhide Yamamoto and A. Marasinghe},
  journal  = {International Journal of Computer Applications},
  year     = {2016},
  pages    = {33-41},
  volume   = {146},
  tasks    = {Part of Speech Tagging},
  abstract = {This paper presents the first part-of-speech (POS) tagging research for Tigrinya (Semitic language) from the newly constructed Nagaoka Tigrinya Corpus. The raw text was extracted from a newspaper published in Eritrea in the Tigrinya language. This initial corpus was cleaned and formatted in plaintext and the Text Encoding Initiative (TEI) XML format. A tagset of 73 tags was designed, and the corpus for POS was manually annotated. This tagset encompasses three levels of grammatical information, which are the main POS categories, subcategories, and POS clitics. The POS tagged corpus contains 72,080 tokens. Tigrinya has a unique pattern of root-template morphology that can be utilized to infer POS categories. Subsequently, a supervised learning approach based on conditional random fields (CRFs) and support vector machines (SVMs) was applied, trained over contextual features of words and POS tags, morphological patterns, and affixes. A rigorous parameter optimization was performed and different combinations of features, data size, and tagsets were experimented upon to boost the overall accuracy, and particularly the prediction of POS for unknown words. For a reduced tagset of 20 tags, an overall accuracy of 90.89\% was obtained on a stratified 10fold cross validation. Enriching contextual features with morphological and affix features improved performance up to 41.01 percentage point, which is significant.}
}

@inproceedings{tedla-etal-2016-segmentation-mt,
  title        = {The effect of shallow segmentation on English-Tigrinya statistical machine translation},
  author       = {Tedla, Yemane and Yamamoto, Kazuhide},
  booktitle    = {2016 International Conference on Asian Language Processing (IALP)},
  year         = {2016},
  organization = {IEEE},
  pages        = {79--82},
  tasks        = {Machine Translation, Morphological Processing},
  abstract     = {This paper presents initial research on English-to-Tigrinya statistical machine translation (SMT). Tigrinya is a highly inflected Semitic language spoken in Eritrea and Ethiopia. Translation involving morphologically complex languages is challenged by factors including data sparseness, word alignment and language model. We try to address these problems through morphological segmentation of Tigrinya words. As a result of segmentation, the size of the language model and its perplexity were greatly reduced. Furthermore, the increase in Tigrinya tokens decreased out-of-vocabulary ratio by 46\%. We analysed phrase-based translation with unsegmented and segmented corpus to investigate the effect of segmentation on translation quality. Preliminary results demonstrate promising performance improvement from a relatively small parallel corpus.}
}

@article{tedla-etal-2017-analyzing-we,
  title    = {Analyzing Word Embeddings and Improving POS Tagger of Tigrinya},
  author   = {Yemane Tedla and Kazuhide Yamamoto},
  journal  = {2017 International Conference on Asian Language Processing (IALP)},
  year     = {2017},
  pages    = {115-118},
  tasks    = {Part of Speech Tagging, Word Embedding},
  abstract = {In this paper, we analyze word embeddings for a morphologically rich language, Tigrinya. Tigrinya is a Semitic language spoken natively in Eritrea and Ethiopia by over seven million people. The unique and complex morphology of Semitic languages, which includes Arabic, Amharic, and Hebrew, is commonly known as 'root and template pattern' morphology. This morphology generates a large number of inflected forms that often cause out-of-vocabulary (OOV) challenges in language processing. This problem is more challenging for low resource languages, such as Tigrinya, that offers very little support of annotated resources. Word embedding methods, given a large raw text corpus, form semantic and syntactic vector representation of words. Therefore, we construct a new text corpus and investigate the optimal settings for generating word vectors for Tigrinya. We also utilize word embeddings to improve the performance of a Tigrinya part-of-speech tagger created from a small tagged corpus.}
}

@inproceedings{tedla-etal-2018-lstm-segmentation,
  title     = {Morphological Segmentation with LSTM Neural Networks for Tigrinya},
  author    = {Yemane Tedla and Kazuhide Yamamoto},
  booktitle = {Intenational Journal on Natural Language Computing (JNLC)},
  journal   = {Intenational Journal on Natural Language Computing (JNLC)},
  year      = {2018},
  volume    = {7},
  tasks     = {Morphological Processing},
  abstract  = {Morphological segmentation is a fundamental task in language processing. Some languages, such as Arabic and Tigrinya, have words packed with very rich morphological information. Therefore, unpacking this information becomes a necessary task for many downstream natural language processing tasks. This paper presents the first morphological segmentation research for Tigrinya. We constructed a new morphologically segmented corpus with about 45,127 manually segmented tokens. Conditional random fields (CRF) and window-based long short-term memory (LSTM) neural networks were employed separately to develop our boundary detection models. We applied language-independent character and substring features for the CRF and character embeddings for the LSTM networks. Experiments were performed with four variants of the Begin-Inside-Outside (BIO) chunk annotation scheme. We achieved 94.67\% F1 score using bidirectional LSTMs with window approach to morpheme boundary detection.}
}

@inproceedings{teklehaymanot-etal-2024-tigqa,
  title     = {{TIGQA}: An Expert-Annotated Question-Answering Dataset in {T}igrinya},
  author    = {Teklehaymanot, Hailay Kidu and Fazlija, Dren and Ganguly, Niloy and Patro, Gourab Kumar and Nejdl, Wolfgang},
  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  month     = {May},
  year      = {2024},
  publisher = {ELRA and ICCL},
  pages     = {16142--16161},
  address   = {Torino, Italia},
  url       = {https://aclanthology.org/2024.lrec-main.1404/},
  tasks     = {Question Answering},
  editor    = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen}
}

@misc{tela-etal-2020-transfer-monolingual-model-preprint,
  title         = {Transferring Monolingual Model to Low-Resource Language: The Case of Tigrinya},
  author        = {Abrhalei Tela and Abraham Woubie and Ville Hautamaki},
  year          = {2020},
  primaryclass  = {cs.CL},
  eprint        = {2006.07698},
  archiveprefix = {arXiv},
  url           = {https://arxiv.org/abs/2006.07698},
  tasks         = {Language Modeling, Sentiment Analysis},
  abstract      = {In recent years, transformer models have achieved great success in natural language processing (NLP) tasks. Most of the current state-of-the-art NLP results are achieved by using monolingual transformer models, where the model is pre-trained using a single language unlabelled text corpus. Then, the model is fine-tuned to the specific downstream task. However, the cost of pre-training a new transformer model is high for most languages. In this work, we propose a cost-effective transfer learning method to adopt a strong source language model, trained from a large monolingual corpus to a low-resource language. Thus, using XLNet language model, we demonstrate competitive performance with mBERT and a pre-trained target language model on the cross-lingual sentiment (CLS) dataset and on a new sentiment analysis dataset for low-resourced language Tigrinya. With only 10k examples of the given Tigrinya sentiment analysis dataset, English XLNet has achieved 78.88\% F1-Score outperforming BERT and mBERT by 10\% and 7\%, respectively. More interestingly, fine-tuning (English) XLNet model on the CLS dataset has promising results compared to mBERT and even outperformed mBERT for one dataset of the Japanese language.},
  tldr          = {This work proposes a cost-effective transfer learning method to adopt a strong source language model, trained from a large monolingual corpus to a low-resource language, and demonstrates competitive performance with mBERT and a pre-trained target language model on the cross-lingual sentiment (CLS) dataset and on a new sentiment analysis dataset for low-resourced language Tigrinya.}
}

@article{tela-etal-2024-transferring-monolingual-model,
  title     = {{Transferring Monolingual Model to Low-Resource Language: The Case of Tigrinya}},
  author    = {Abrhalei Tela and Abraham Woubie and Ville Hautam\"aki},
  journal   = {Applied Computing and Intelligence},
  year      = {2024},
  publisher = {AIMS Press},
  pages     = {184--194},
  volume    = {4},
  doi       = {10.3934/aci.2024011},
  url       = {https://www.aimspress.com/article/doi/10.3934/aci.2024011},
  tasks     = {Language Modeling, Sentiment Analysis},
  abstract  = {In recent years, transformer models have achieved great success in natural language processing (NLP) tasks. Most of the current results are achieved by using monolingual transformer models, where the model is pre-trained using a single-language unlabelled text corpus. Then, the model is fine-tuned to the specific downstream task. However, the cost of pre-training a new transformer model is high for most languages. In this work, we propose a cost-effective transfer learning method to adopt a strong source language model, trained from a large monolingual corpus to a low-resource language. Thus, using the XLNet language model, we demonstrate competitive performance with mBERT and a pre-trained target language model on the cross-lingual sentiment (CLS) dataset and on a new sentiment analysis dataset for the low-resource language Tigrinya. With only 10k examples of the given Tigrinya sentiment analysis dataset, English XLNet achieved 78.88\% F1-Score, outperforming BERT and mBERT by 10\% and 7\%, respectively. More interestingly, fine-tuning (English) XLNet model on the CLS dataset showed promising results compared to mBERT and even outperformed mBERT for one dataset of the Japanese language.},
  issn      = {2771-392X},
  number    = {2},
  preprint  = {https://arxiv.org/abs/2006.07698}
}

@article{tesfagergish-2020-pos-dnn,
  title    = {Part-of-Speech Tagging via Deep Neural Networks for Northern-Ethiopic Languages},
  author   = {Senait Gebremichael Tesfagergish and Jurgita Kapociute-Dzikiene},
  journal  = {Information Technology and Control},
  year     = {2020},
  pages    = {482-494},
  volume   = {49},
  tasks    = {Part of Speech Tagging},
  abstract = {Deep Neural Networks (DNNs) have proven to be especially successful in the area of Natural Language Processing (NLP) and Part-Of-Speech (POS) tagging—which is the process of mapping words to their corresponding POS labels depending on the context. Despite recent development of language technologies, low-resourced languages (such as an East African Tigrinya language), have received too little attention. We investigate the effectiveness of Deep Learning (DL) solutions for the low-resourced Tigrinya language of the Northern-Ethiopic branch. We have selected Tigrinya as the testbed example and have tested state-of-the-art DL approaches seeking to build the most accurate POS tagger. We have evaluated DNN classifiers (Feed Forward Neural Network – FFNN, Long Short-Term Memory method – LSTM, Bidirectional LSTM, and Convolutional Neural Network – CNN) on a top of neural word2vec word embeddings with a small training corpus known as Nagaoka Tigrinya Corpus. To determine the best DNN classifier type, its architecture and hyper-parameter set both manual and automatic hyper-parameter tuning has been performed. BiLSTM method was proved to be the most suitable for our solving task: it achieved the highest accuracy equal to 92\% that is 65\% above the random baseline.}
}

@mastersthesis{tsegaye-2014-factored-mt,
  title    = {English-Tigrigna Factored Statistical Machine Translation},
  author   = {Tarik Tsegaye},
  journal  = {Master's Thesis, submitted to Addis Ababa University},
  year     = {2014},
  school   = {Addis Ababa University},
  url      = {https://etd.aau.edu.et/items/ba6aa8e7-3d08-4ac4-9aea-87bed0b01a90},
  tasks    = {Machine Translation},
  abstract = {A thesis submitted to the school of graduate studies of Addis Ababa University in partial fulfillment of the requirements of the degree of Master of Science in Information Science}
}

@inproceedings{yohannes-etal-2022-ner,
  title     = {Named-Entity Recognition for a Low-Resource Language Using Pre-Trained Language Model},
  author    = {Yohannes, Hailemariam Mehari and Amagasa, Toshiyuki},
  booktitle = {Proceedings of the 37th SIGAPP Symposium on Applied Computing},
  year      = {2022},
  location  = {Virtual Event},
  numpages  = {8},
  publisher = {Association for Computing Machinery},
  series    = {SAC '22},
  pages     = {837--844},
  address   = {New York, NY, USA},
  doi       = {10.1145/3477314.3507066},
  isbn      = {9781450387132},
  url       = {https://doi.org/10.1145/3477314.3507066},
  tasks     = {Named Entity Recognition},
  abstract  = {This paper proposes a method for Named-Entity Recognition (NER) for a low-resource language, Tigrinya, using a pre-trained language model. Tigrinya is a morphologically rich language, although one of the underrepresented in the field of NLP. This is mainly due to the limited amount of annotated data available. To address this problem, we introduced the first publicly available NER dataset for Tigrinya. The dataset contains 69,309 tokens that were manually annotated based on the CoNLL 2003 Beginning, Inside, and Outside (BIO) tagging schema. Specifically, we develop a new pre-trained language model for Tigrinya based on RoBERTa, which we refer to as TigRoBERTa. First, It is trained on an unsupervised Tigrinya corpus using Masked Language Modeling (MLM). Then, we show the validity of TigRoBERTa by fine-tuning for a couple of downstream tasks, namely, NER and Part of Speech (POS) tagging. The experimental results show that the method achieved 81.05\% F1-score for NER and 92\% accuracy for POS tagging, which is better than or comparable to the baseline method based on the CNN-BiLSTM-CRF model.}
}
