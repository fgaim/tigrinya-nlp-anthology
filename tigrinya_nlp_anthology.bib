@inproceedings{abate-etal-2018-parallel-corpora,
  title     = {Parallel Corpora for bi-lingual English-Ethiopian Languages Statistical Machine Translation},
  author    = {Solomon Teferra Abate and Michael Melese Woldeyohannis and Martha Yifiru Tachbelie and Million Meshesha and Solomon Atinafu and Wondwossen Mulugeta and Yaregal Assabie and Hafte Abera and Binyam Ephrem Seyoum and Tewodros Abebe and Wondimagegnhue Tsegaye and Amanuel Lemma and Tsegaye Andargie and Seifedin Shifaw},
  booktitle = {International Conference on Computational Linguistics},
  year      = {2018},
  tasks     = {Machine Translation},
  abstract  = {In this paper, we describe an attempt towards the development of parallel corpora for English and Ethiopian Languages, such as Amharic, Tigrigna, Afan-Oromo, Wolaytta and Ge’ez. The corpora are used for conducting a bi-directional statistical machine translation experiments. The BLEU scores of the bi-directional Statistical Machine Translation (SMT) systems show a promising result. The morphological richness of the Ethiopian languages has a great impact on the performance of SMT specially when the targets are Ethiopian languages. Now we are working towards an optimal alignment for a bi-directional English-Ethiopian languages SMT.}
}

@inproceedings{abate-etal-2019-english-ethiopian-smt,
  title     = {English-Ethiopian Languages Statistical Machine Translation},
  author    = {Solomon Teferra Abate and Michael Melese and Martha Yifiru Tachbelie and Million Meshesha and Solomon Atinafu and Wondwossen Mulugeta and Yaregal Assabie and Hafte Abera and Biniyam Ephrem and Tewodros Gebreselassie and Wondimagegnhue Tsegaye Tufa and Amanuel Lemma and Tsegaye Andargie and Seifedin Shifaw},
  booktitle = {WNLP Workshop at ACL},
  year      = {2019},
  tasks     = {Machine Translation},
  abstract  = {In this paper, we describe an attempt towards the development of parallel corpora for English and Ethiopian Languages, such as Amharic, Tigrigna, Afan-Oromo, Wolaytta and Ge’ez. The corpora are used for conducting bi-directional SMT experiments. The BLEU scores of the bi-directional SMT systems show a promising result. The morphological richness of the Ethiopian languages has a great impact on the performance of SMT especially when the targets are Ethiopian languages.}
}

@inproceedings{abdelkadir-etal-2023-mt-error,
  title     = {Error Analysis of {Tigrinya - English} Machine Translation Systems},
  author    = {Nuredin Ali Abdelkadir and Negasi Haile Abadi and Asmelash Teka Hadgu},
  booktitle = {Proceedings of the 4th Workshop on African Natural Language Processing, AfricaNLP@ICLR 2023, Kigali, Rwanda, May 1, 2023},
  year      = {2023},
  url       = {https://openreview.net/pdf?id=BQVqNyzCxx},
  tasks     = {Machine Translation},
  timestamp = {Wed, 06 Sep 2023 12:12:32 +0200}
}

@inproceedings{abera-h-mariam-2018-design,
  title     = {Design of a {T}igrinya Language Speech Corpus for Speech Recognition},
  author    = {Abera, Hafte  and H/Mariam, Sebsibe},
  booktitle = {Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing},
  month     = {August},
  year      = {2018},
  publisher = {Association for Computational Linguistics},
  pages     = {78--82},
  address   = {Santa Fe, New Mexico, USA},
  url       = {https://aclanthology.org/W18-3811},
  tasks     = {Speech Recognition},
  abstract  = {In this paper, we describe the first Tigrinya Languages speech corpora designed and development for speech recognition purposes. Tigrinya, often written as Tigrigna (ትግርኛ) /tɪˈɡrinjə/ belongs to the Semitic branch of the Afro-Asiatic languages where it shows the characteristic features of a Semitic language. It is spoken by ethnic Tigray-Tigrigna people in the Horn of Africa. The paper outlines different corpus designing process analysis of related work on speech corpora creation for different languages. The authors provide also procedures that were used for the creation of Tigrinya speech recognition corpus which is the under-resourced language. One hundred and thirty speakers, native to Tigrinya language, were recorded for training and test dataset set. Each speaker read 100 texts, which consisted of syllabically rich and balanced sentences. Ten thousand sets of sentences were used to prompt sheets. These sentences contained all of the contextual syllables and phones.}
}

@inproceedings{abera-h-mariam-2019-speech,
  title     = {Speech Recognition for {T}igrinya language Using Deep Neural Network Approach},
  author    = {Abera, Hafte  and H/mariam, Sebsibe},
  booktitle = {Proceedings of the 2019 Workshop on Widening NLP},
  month     = {August},
  year      = {2019},
  publisher = {Association for Computational Linguistics},
  pages     = {7--9},
  address   = {Florence, Italy},
  url       = {https://aclanthology.org/W19-3603},
  tasks     = {Speech Recognition},
  abstract  = {This work presents a speech recognition model for Tigrinya language .The Deep Neural Network is used to make the recognition model. The Long Short-Term Memory Network (LSTM), which is a special kind of Recurrent Neural Network composed of Long Short-Term Memory blocks, is the primary layer of our neural network model. The 40-dimensional features are MFCC-LDA-MLLT-fMLLR with CMN were used. The acoustic models are trained on features that are obtained by projecting down to 40 dimensions using linear discriminant analysis (LDA). Moreover, speaker adaptive training (SAT) is done using a single feature-space maximum likelihood linear regression (FMLLR) transform estimated per speaker. We train and compare LSTM and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models. Finally, the accuracy of the model is evaluated based on the recognition rate.}
}

@inproceedings{adelani-etal-2023-masakhanews,
  title     = {{M}asakha{NEWS}: News Topic Classification for {A}frican languages},
  author    = {Adelani, David Ifeoluwa  and Masiak, Marek  and Azime, Israel Abebe  and Alabi, Jesujoba  and Tonja, Atnafu Lambebo  and Mwase, Christine  and Ogundepo, Odunayo  and Dossou, Bonaventure F. P.  and Oladipo, Akintunde  and Nixdorf, Doreen  and Emezue, Chris Chinenye  and Al-azzawi, Sana  and Sibanda, Blessing  and David, Davis  and Ndolela, Lolwethu  and Mukiibi, Jonathan  and Ajayi, Tunde  and Moteu, Tatiana  and Odhiambo, Brian  and Owodunni, Abraham  and Obiefuna, Nnaemeka  and Mohamed, Muhidin  and Muhammad, Shamsuddeen Hassan  and Ababu, Teshome Mulugeta  and Salahudeen, Saheed Abdullahi  and Yigezu, Mesay Gemeda  and Gwadabe, Tajuddeen  and Abdulmumin, Idris  and Taye, Mahlet  and Awoyomi, Oluwabusayo  and Shode, Iyanuoluwa  and Adelani, Tolulope  and Abdulganiyu, Habiba  and Omotayo, Abdul-Hakeem  and Adeeko, Adetola  and Afolabi, Abeeb  and Aremu, Anuoluwapo  and Samuel, Olanrewaju  and Siro, Clemencia  and Kimotho, Wangari  and Ogbu, Onyekachi  and Mbonu, Chinedu  and Chukwuneke, Chiamaka  and Fanijo, Samuel  and Ojo, Jessica  and Awosan, Oyinkansola  and Kebede, Tadesse  and Sakayo, Toadoum Sari  and Nyatsine, Pamela  and Sidume, Freedmore  and Yousuf, Oreen  and Oduwole, Mardiyyah  and Tshinu, Kanda  and Kimanuka, Ussen  and Diko, Thina  and Nxakama, Siyanda  and Nigusse, Sinodos  and Johar, Abdulmejid  and Mohamed, Shafie  and Hassan, Fuad Mire  and Mehamed, Moges Ahmed  and Ngabire, Evrard  and Jules, Jules  and Ssenkungu, Ivan  and Stenetorp, Pontus},
  booktitle = {Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {November},
  year      = {2023},
  publisher = {Association for Computational Linguistics},
  pages     = {144--159},
  address   = {Nusa Dua, Bali},
  doi       = {10.18653/v1/2023.ijcnlp-main.10},
  url       = {https://aclanthology.org/2023.ijcnlp-main.10/},
  tasks     = {Topic Classification},
  abstract  = {African languages are severely under-represented in NLP research due to lack of datasets covering several NLP tasks. While there are individual language specific datasets that are being expanded to different tasks, only a handful of NLP tasks (e.g. named entity recognition and machine translation) have standardized benchmark datasets covering several geographical and typologically-diverse African languages. In this paper, we develop MasakhaNEWS -- a new benchmark dataset for news topic classification covering 16 languages widely spoken in Africa. We provide an evaluation of baseline models by training classical machine learning models and fine-tuning several language models. Furthermore, we explore several alternatives to full fine-tuning of language models that are better suited for zero-shot and few-shot learning such as cross-lingual parameter-efficient fine-tuning (like MAD-X), pattern exploiting training (PET), prompting language models (like ChatGPT), and prompt-free sentence transformer fine-tuning (SetFit and Cohere Embedding API). Our evaluation in zero-shot setting shows the potential of prompting ChatGPT for news topic classification in low-resource African languages, achieving an average performance of 70 F1 points without leveraging additional supervision like MAD-X. In few-shot setting, we show that with as little as 10 examples per label, we achieved more than 90\% (i.e. 86.0 F1 points) of the performance of full supervised training (92.6 F1 points) leveraging the PET approach.},
  editor    = {Park, Jong C.  and Arase, Yuki  and Hu, Baotian  and Lu, Wei  and Wijaya, Derry  and Purwarianti, Ayu  and Krisnadhi, Adila Alfa}
}

@mastersthesis{amare-2016-question,
  title    = {Tigrigna Question Answering System for Factoid Questions},
  author   = {Kibrom Haftu Amare},
  journal  = {Master's Thesis, submitted to Addis Ababa University},
  month    = {June},
  year     = {2016},
  school   = {Addis Ababa University},
  url      = {http://etd.aau.edu.et/handle/123456789/2375},
  tasks    = {Machine Translation, Question Answering},
  abstract = {Accessing relevant information is one of the major problems faced by Tigrigna language users for every domain of knowledge when dealing with huge amount of information especially in the Internet. Evidently, users are interested in obtaining a specific and precise answer to a specific question. However, obtaining a relevant and concise answer is a challenge to particular user question. For such situation, Tigrigna Question Answering system is a good solution. The proposed QA system comprises of question analysis, document analysis and answer extraction modules. The main function of question analysis module is taking a Tigrigna Question as input and then generates a query, expands a query and determines its Question Particle and Question Type. A statistical language model approach is used to model the classification of Tigrigna questions to their category or type. The document analysis module performs the process of pre-processing of parallel corpora, which are documents that contain question sentences in one document and answer sentences in another one, and also ranking and extracting answer contents. Answer extraction also performs the detail analysis on the retrieved answer contents based on the question type, question particle and query using the techniques of language modeling called Answer Model. This statistical language model does the extraction process of exact and precise Tigrigna answer in probabilistic manner from sets candidate answers. Generally, this system developed after reviewed literatures and related work, and selected the appropriate tools and data source such as Moses, GIZA++ and IRSTLM as tools and different Webs and Tigrigna newspapers and magazines as data sources. Our data sets are classified for training and testing activities of the system. Based on this, we collected around 1000 data sets for training and 200 data sets for testing. Performance evaluation conducted manually by comparing the system‟s answers with the answers exists in testing document, which is prepared for testing purpose. Finally the evaluation results of Tigrigna factoid QAS is expressed in terms of the average performance of a question type classifier which is 87%, and the average Precision, Recall and F – measure of the answer extraction, precision is 88.5%, recall is 85.9% and F – measure is 87.2%. Keywords: Tigrigna question answering, Tigrigna Factoid questions, Language model based question classification, question analysis, Document Analysis, Answer Extraction.}
}

@mastersthesis{asfaw-2018-lid,
  title    = {A Comparative Study of Automatic Language Identification on Ethio-Semitic Languages},
  author   = {Rediat Bekele Asfaw},
  journal  = {Master's Thesis, submitted to Addis Ababa University},
  month    = {June},
  year     = {2018},
  school   = {Addis Ababa University},
  doi      = {http://dx.doi.org/10.13140/RG.2.2.10377.70245},
  tasks    = {Language Identification},
  abstract = {The dominant languages under the family of Ethio-Semitic languages are Amharic, Geez, Guragigna and Tigrigna. From the findings of the language identification studies on European languages, there is a conclusion that most classifiers performance reached the accuracy of 100%. Local and global studied confirmed that Naïve Bayes Classifier (NBC) classifier does not reached the accuracy level of 100% in language identification especially on shorter test strings. Comparative Language Identification studies in European languages shows that Cumulative Frequency Addition (CFA) performs close to 100% accuracies better than the NBC classifier. The purpose of our study is to assess the performance of CFA as compared to NBC on Ethio-Semitic languages, to validate the research findings of CFA and NBC classifiers, and recommend the classifier, language model, evaluation context and the optimal values of N that performs better in language identification. In this research we have employed and experimental study to measure the performance CFA and NBC classifiers. We have developed a training and test corpus from online bibles written in Amharic, Geez, Guragigna and Tigrigna to generate 5 different character-based n-gram language models. We have measured the classifiers performance using under two different evaluation contexts using 10-fold cross validation. F-score is used as an optimal measure of performance for comparing classifiers performances. The classifiers commonly exhibited higher performance when the length of the test phrase grows from a single word to 2, 3 and beyond to reach an F-score measure beyond 99%. Both classifiers performed similarly under each context corresponding to the language models and n-grams tested. The language model, fixed length character n-grams with location features, exhibited highest performance in F-score for both classifiers under each evaluation contexts on test strings as short as one-word length. N=5 on Fixed length character n-grams with location features language model is the optimal value of N whereas N=2 is the optimal value for the remaining language models on both CFA and NBC classifiers and evaluation contexts. Based on our findings CFA is a classifier that performs better as compared to NBC as it is founded in sound theoretical assumptions and its performance in language identification.}
}

@article{azath-etal-2020-smt,
  title    = {Statistical Machine Translator For English To Tigrigna Translation},
  author   = {Azath and Tsegay Kiros},
  journal  = {International Journal of Scientific & Technology Research},
  year     = {2020},
  pages    = {2095-2099},
  volume   = {9},
  tasks    = {Machine Translation},
  abstract = {Machine Translation is the automatic translation of text from a source language to the target language. The demand for translation has been increasing due to the exchange of information between various regions using different regional languages. English-Tigrigna Statistical Machine Translation, therefore, is required since a lot of documents are written in English. This research study used statistical machine translation approach due to it yields high accuracy and does not need linguistic rules which exploit human effort (knowledge). The language model, Translation model, and decoder are the three basic components in Statistical Machine Translation (SMT). Moses' decoder, Giza++, IRSTLM, and BLEU (Bilingual Evaluation Understudy) are tools that helped to conduct the experiments. 17,338 sentences of bilingual corpus for training, 1000 sentences for test set and 42,284 sentences for language model were used for experiment. The BLEU score produced from the experiment was 23.27\% which would still not enough for applicable applications. As a result, the effect of word factored or segmentation in the translation quality is reduced by increasing the data size of the corpus.}
}

@mastersthesis{bahre-2022-hate,
  title    = {Hate Speech Detection from Facebook Social Media Posts and Comments in Tigrigna language},
  author   = {Weldemariam Bahre},
  journal  = {Master's Thesis, submitted to St. Mary's University},
  month    = {June},
  year     = {2022},
  school   = {The faculty of informatics, St. Mary's University},
  url      = {http://repository.smuc.edu.et/handle/123456789/6929},
  tasks    = {Hate/Abusiveness Detection},
  abstract = {In recent years, hate speech on social media has become a common phenomenon in the Ethiopian online community particularly due to the substantial growth of users. As part of our country language Tigrigna language Facebook users also increased in recent years. In line with this, the hate speech in Tigrigna language is also increased. The reason could be due to, the political instabilities. Hate speech on social media has the potential to quickly disseminate through the online users that could escalate an act of violence and hate crime among peoples. To address this problem, this research proposed hate speech detection using machine learning and text-mining feature extraction techniques to build a detection model. A hate speech data written in Tigrigna language was collected from the Facebook public page and manually labeled into hate and hate-free classes to build binary class datasets. The research employed an experimental approach to determine the best combination of the machine learning algorithm and features extraction for modeling. Support Vector Machine (SVM), Naïve Bayes (NB) and Random Forest (RF)classification algorithms are employed to construct hate speech detection model using the whole dataset with the extracted features based on word unigram, bigram, trigram, as well as combined n-grams and TF*IDF. An experimental result shows that the Naïve Bayes classification algorithm with TF*DF feature extraction were achieved slightly better performance than the SVM and RF models for hate speech detection with 79% accuracy. In this study we achieved a promising result for designing hate speech detection for Tigrigna language. Since there is no data set available for experimentation, we used limited data for constructing an optimal hate speech detection model using machine learning classification algorithm. Hence, we recommend the need to prepare standard corpus for hate speech detection in local languages, including Tigrigna language.}
}

@inproceedings{bandarkar-etal-2024-belebele,
  title     = {The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants},
  author    = {Bandarkar, Lucas and Liang, Davis and Muller, Benjamin and Artetxe, Mikel and Shukla, Satya Narayan and Husa, Donald and Goyal, Naman and Krishnan, Abhinandan and Zettlemoyer, Luke and Khabsa, Madian},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {August},
  year      = {2024},
  publisher = {Association for Computational Linguistics},
  pages     = {749--775},
  address   = {Bangkok, Thailand},
  doi       = {10.18653/v1/2024.acl-long.44},
  url       = {https://aclanthology.org/2024.acl-long.44/},
  tasks     = {Question Answering},
  abstract  = {We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the Flores-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and find that despite significant cross-lingual transfer in English-centric LLMs, much smaller MLMs pretrained on balanced multilingual data still understand far more languages. We also observe that larger vocabulary size and conscious vocabulary construction correlate with better performance on low-resource languages. Overall, Belebele opens up new avenues for evaluating and analyzing the multilingual capabilities of NLP systems.},
  editor    = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek}
}

@article{berhane-etal-2025-tinc24,
  title     = {{Towards Neural Named Entity Recognition System in Tigrinya with Large-scale Dataset}},
  author    = {Sham K. Berhane and Simon M. Beyene and Yoel G. Teklit and Ibrahim A. Ibrahim and Natnael A. Teklu and Sirak A. Bereketeab and Fitsum Gaim},
  journal   = {Language Resources and Evaluation},
  month     = {June},
  year      = {2025},
  publisher = {Springer Nature},
  pages     = {},
  volume    = {},
  doi       = {10.1007/s10579-025-09825-4},
  url       = {https://doi.org/10.1007/s10579-025-09825-4},
  tasks     = {Named Entity Recognition, Part of Speech Tagging},
  issn      = {1574-0218},
  number    = {}
}

@inproceedings{berihu-2020-mt,
  title     = {Enhancing Bi-directional English-Tigrigna Machine Translation Using Hybrid Approach},
  author    = {Zemicheal Berihu and Gebremariam Mesfin and Mulugeta Atsibaha and Tor-Morten and Gr{\o}nli},
  booktitle = {},
  year      = {2020},
  tasks     = {Machine Translation},
  abstract  = {Machine Translation (MT) is an application area of NLP where automatic systems are used to translate text or speech from one language to another while preserving the meaning of the source language. Although there exists a large volume of literature in automatic machine translation of documents in many languages, the translation between English and Tigrigna is less explored. Therefore, we proposed the hybrid approach to address the challenges of applying syntactic reordering rules which align and capture the structural arrangement of words in the source sentence to become more like the target sentences. Two language models were developedone for English and another for Tigrigna and about 12,000 parallel sentences in four domains and 32,000 bilingual dictionaries were collected for our experiment. The parallel collected corpus was split randomly to 10,800 sentences for training set and 1,200 sentences for testing. Moses open source statistical machine translation system has been used for the experiment to train, tune and decode. The parallel corpus was aligned using the Giza++ toolkit and SRILM was used for building the language model. Three main experiments were conducted using statistical approach, hybrid approach and post-processing technique. According to our experimental result showed good translation output as high as 32.64 BLEU points Google translator and the hybrid approach was found most promising for English-Tigrigna bi-directional translation.}
}

@inproceedings{feleke-2017-similarity,
  title     = {The similarity and Mutual Intelligibility between {A}mharic and {T}igrigna Varieties},
  author    = {Feleke, Tekabe Legesse},
  booktitle = {Proceedings of the Fourth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial)},
  month     = {April},
  year      = {2017},
  publisher = {Association for Computational Linguistics},
  pages     = {47--54},
  address   = {Valencia, Spain},
  doi       = {10.18653/v1/W17-1206},
  url       = {https://aclanthology.org/W17-1206},
  tasks     = {Language Identification},
  abstract  = {The present study has examined the similarity and the mutual intelligibility between Amharic and Tigrigna using three tools namely Levenshtein distance, intelligibility test and questionnaires. The study has shown that both Tigrigna varieties have almost equal phonetic and lexical distances from Amharic. The study also indicated that Amharic speakers understand less than 50{\%} of the two varieties. Furthermore, the study showed that Amharic speakers are more positive about the Ethiopian Tigrigna variety than the Eritrean Variety. However, their attitude towards the two varieties does not have an impact on their intelligibility. The Amharic speakers{'} familiarity to the Tigrigna varieties is largely dependent on the genealogical relation between Amharic and the two Tigrigna varieties.}
}

@article{fesseha-etal-2021-text-classification,
  title          = {Text Classification Based on Convolutional Neural Networks and Word Embedding for Low-Resource Languages: Tigrinya},
  author         = {Fesseha, Awet and Xiong, Shengwu and Emiru, Eshete Derb and Diallo, Moussa and Dahou, Abdelghani},
  journal        = {Information},
  year           = {2021},
  volume         = {12},
  doi            = {10.3390/info12020052},
  url            = {https://www.mdpi.com/2078-2489/12/2/52},
  tasks          = {Topic Classification},
  abstract       = {This article studies convolutional neural networks for Tigrinya (also referred to as Tigrigna), which is a family of Semitic languages spoken in Eritrea and northern Ethiopia. Tigrinya is a “low-resource” language and is notable in terms of the absence of comprehensive and free data. Furthermore, it is characterized as one of the most semantically and syntactically complex languages in the world, similar to other Semitic languages. To the best of our knowledge, no previous research has been conducted on the state-of-the-art embedding technique that is shown here. We investigate which word representation methods perform better in terms of learning for single-label text classification problems, which are common when dealing with morphologically rich and complex languages. Manually annotated datasets are used here, where one contains 30,000 Tigrinya news texts from various sources with six categories of “sport”, “agriculture”, “politics”, “religion”, “education”, and “health” and one unannotated corpus that contains more than six million words. In this paper, we explore pretrained word embedding architectures using various convolutional neural networks (CNNs) to predict class labels. We construct a CNN with a continuous bag-of-words (CBOW) method, a CNN with a skip-gram method, and CNNs with and without word2vec and FastText to evaluate Tigrinya news articles. We also compare the CNN results with traditional machine learning models and evaluate the results in terms of the accuracy, precision, recall, and F1 scoring techniques. The CBOW CNN with word2vec achieves the best accuracy with 93.41%, significantly improving the accuracy for Tigrinya news classification.},
  article-number = {52},
  issn           = {2078-2489},
  number         = {2}
}

@mastersthesis{gaim-2017-morphology-mt,
  title    = {{Applying Morphological Segmentation to Machine Translation of Low-Resourced and Morphologically Complex Languages: The case of Tigrinya}},
  author   = {Fitsum Gaim},
  journal  = {Master's Thesis, submitted to Korea Advanced Institute of Science and Technology ({KAIST})},
  month    = {July},
  year     = {2017},
  school   = {School of Computing, Korea Advanced Institute of Science and Technology ({KAIST})},
  tasks    = {Machine Translation, Morphological Processing},
  abstract = {Machine Translation (MT) has seen substantial advances in recent years, but it remains unexplored and an ongoing challenge for most language pairs. In this thesis, we present the development of a Statistical Machine Translation (SMT) between English and a lesser-known Semitic language, Tigrinya. To the best of our knowledge, this is the first study of machine translation involving the Tigrinya language. Two of the most important factors that affect the performance of SMT systems for a given language pair are: (1) the volume of parallel data available, and (2) the language difference between the pair. In this regard, English and Tigrinya make a particularly difficult pair for the task of SMT. The English language is deeply studied and has a wealth of resources, whereas Tigrinya is much less studied with severely limited computational resources. What is more, the two languages differ markedly in syntax and morphology, particularly in the word structure. Tigrinya is an agglutinative language with a highly derivational and inflectional morphology that proliferates vocabulary and necessitates sub-word translation. Regardless of the salient differences in the making of a word among natural languages, the standard SMT approaches treat surface words as the smallest unit of translation. These techniques work fairly well for languages with simple morphology and relatively small vocabulary such as English. However, they perform suboptimal when languages with rich morphology and huge vocabulary are involved, owing it to poor phrase alignment, data sparsity, and high rate of out-of- vocabulary words. In this empirical study, we build the necessary corpora from scratch and study the effects of both rule-based and unsupervised morphological segmentation of Tigrinya words as remedial measures. Moreover, we augment the system with additional bilingual lexicon to ameliorate the out-of-vocabulary problem. To this end, we have achieved cumulative BLEU scores of 23.3 and 27.14 points for English into Tigrinya, and Tigrinya into English translations, respectively. In the end, the system is published online for public use and the dataset, which comprises 30.6k sentences of parallel corpus and 913k sentences of monolingual corpus, is also made publicly available for researchers.}
}

@misc{gaim-2021-glocr,
  title     = {{GLOCR: GeezLab OCR Dataset}},
  author    = {Fitsum Gaim},
  month     = {June},
  year      = {2021},
  publisher = {Harvard Dataverse},
  version   = {1.1},
  doi       = {10.7910/DVN/RQTSD2},
  url       = {https://doi.org/10.7910/DVN/RQTSD2},
  dataverse = {https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/RQTSD2},
  tasks     = {Optical Character Recognition},
  abstract  = {A Text Recognition (TR) and Optical Character Recognition (OCR) dataset for the Tigrinya language. The dataset contains a total of 710k image-label pairs from multiple data sources. In addition to the characters-only data, the major part of the dataset is a collection of multi-word text images with labels from three categories: News (from Haddas Ertra newspaper), the Bible, and random-trigrams of the 150k most common words in Tigrinya.}
}

@misc{gaim-2024-semantic-search,
  title     = {{Semantic Search Models for Tigrinya}},
  author    = {Fitsum Gaim},
  month     = {January},
  year      = {2024},
  publisher = {Hugging Face Hub},
  doi       = {10.57967/hf/6068},
  url       = {https://huggingface.co/fgaim/tiroberta-bi-encoder},
  tasks     = {Information Retrieval},
  abstract  = {This work introduces monolingual bi-encoder language models for Tigrinya. The models are based on the TiRoBERTa and TiELECTRA architectures and are trained on Tigrinya question-answering and information retrieval datasets. The models are designed to support semantic search tasks, such as information retrieval, text representation, and question answering.}
}

@inproceedings{gaim-etal-2021-tiplms,
  title     = {Monolingual Pre-trained Language Models for Tigrinya},
  author    = {Fitsum Gaim and Wonsuk Yang and Jong C. Park},
  booktitle = {5th Widening NLP (WiNLP2021) workshop, co-located with the 2021 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  year      = {2021},
  url       = {http://www.winlp.org/wp-content/uploads/2021/11/winlp2021_62_Paper.pdf},
  tasks     = {Language Modeling},
  abstract  = {Pre-trained language models (PLMs) are driving much of the recent progress in natural language processing. However, due to the resource-intensive nature of the models, underrepresented languages without sizable curated data have not seen significant progress. Multilingual PLMs have been introduced with the potential to generalize across many languages, but their performance trails compared to their monolingual counterparts and depends on the characteristics of the target language. In the case of the Tigrinya language, recent studies report a sub-optimal performance when applying the current multilingual models. This may be due to its orthography and unique linguistic characteristics, especially when compared to the Indo-European and other typologically distant languages that were used to train the models. In this work, we pre-train three monolingual PLMs for Tigrinya on a newly compiled corpus, and we compare the models with their multilingual counterparts on two downstream tasks, part-of-speech tagging and sentiment analysis, achieving significantly better results and establishing the state-of-the-art. We make the data and trained models publicly available.}
}

@article{gaim-etal-2021-tlmd,
  title     = {TLMD: Tigrinya Language Modeling Dataset},
  author    = {Fitsum Gaim and Wonsuk Yang and Jong C. Park},
  journal   = {Zenodo},
  month     = {July},
  year      = {2021},
  publisher = {Zenodo},
  version   = {1.0.0},
  doi       = {10.5281/zenodo.5139094},
  url       = {https://doi.org/10.5281/zenodo.5139094},
  tasks     = {Language Modeling},
  abstract  = {A monolingual dataset built for Tigrinya language modeling. To the best of our knowledge, this is the largest dataset for Tigrinya of its kind. The data was collected from various sources across the web including news, blogs, and books. The largest portion of the data, ~75\%, comes from over 2150 issues of the Haddas Ertra newspaper and other magazines published by www.shabait.com. Data Statistics: Total size: ~0.5GB; Around 40 million tokens; Over 2 million lines; 367 unique characters; Train split: 98\%, 1.97 million lines; Validation split: 2\%, 43k lines. We have done a light-weight cleanup of the data: - Removal of Tigrinya text with legacy and non-standard encoding systems. - Normalization of punctuation and special characters. - Removal of redundant white spaces and empty lines. - Rejoining or fixing broken sentences when possible. - Removal of foreign words. We avoid applying any form of tokenization, extensive cleanup, and preprocessing operations in order not to take away potentially useful information, those decisions are left to the use-case researchers or developers.}
}

@misc{gaim-etal-2022-analogy-test,
  title     = {{Tigrinya Analogy Test for evaluating Word Embeddings}},
  author    = {Fitsum Gaim and Jong C. Park},
  month     = {May},
  year      = {2022},
  publisher = {Zenodo},
  version   = {1.0.0},
  doi       = {10.5281/zenodo.7089051},
  url       = {https://doi.org/10.5281/zenodo.7089051},
  github    = {https://github.com/fgaim/tigrinya-analogy-test},
  tasks     = {Word Embedding},
  abstract  = {This is a Tigrinya version of the Google Analogy Test set, which is used to evaluate English word-embedding models. The analogy test is a well-established strategy to empirically evaluate the quality of word-embedding models. More information about the English task can be found at the ACL Wiki. The data was first machine translated then manually verified by a native speaker to reduce errors. Some aspects of the original analogy test is focused on English and may not transfer well to other languages, such as those related to grammar or morphology. Therefore, when adapting the task we have discarded examples that were discovered as irrelevant in Tigrinya. Finally, there are a total of 18,465 entries in the Tigrinya Analogy Test set, while the source English data has 19,544 entries.}
}

@inproceedings{gaim-etal-2022-geezswitch,
  title     = {{G}eez{S}witch: Language Identification in Typologically Related Low-resourced {E}ast {A}frican Languages},
  author    = {Fitsum Gaim and Wonsuk Yang and Jong C. Park},
  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
  month     = {June},
  year      = {2022},
  publisher = {European Language Resources Association},
  pages     = {6578--6584},
  address   = {Marseille, France},
  url       = {https://aclanthology.org/2022.lrec-1.707},
  tasks     = {Language Identification},
  abstract  = {Language identification is one of the fundamental tasks in natural language processing that is a prerequisite to data processing and numerous applications. Low-resourced languages with similar typologies are generally confused with each other in real-world applications such as machine translation, affecting the user{'}s experience. In this work, we present a language identification dataset for five typologically and phylogenetically related low-resourced East African languages that use the Ge{'}ez script as a writing system; namely Amharic, Blin, Ge{'}ez, Tigre, and Tigrinya. The dataset is built automatically from selected data sources, but we also performed a manual evaluation to assess its quality. Our approach to constructing the dataset is cost-effective and applicable to other low-resource languages. We integrated the dataset into an existing language-identification tool and also fine-tuned several Transformer based language models, achieving very strong results in all cases. While the task of language identification is easy for the informed person, such datasets can make a difference in real-world deployments and also serve as part of a benchmark for language understanding in the target languages. The data and models are made available at https://github.com/fgaim/geezswitch.}
}

@inproceedings{gaim-etal-2023-question,
  title     = {Question-Answering in a Low-resourced Language: Benchmark Dataset and Models for {T}igrinya},
  author    = {Fitsum Gaim and Wonsuk Yang and Hancheol Park and Jong C. Park},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2023},
  publisher = {Association for Computational Linguistics},
  pages     = {11857--11870},
  address   = {Toronto, Canada},
  url       = {https://aclanthology.org/2023.acl-long.661},
  tasks     = {Question Answering},
  abstract  = {Question-Answering (QA) has seen significant advances recently, achieving near human-level performance over some benchmarks. However, these advances focus on high-resourced languages such as English, while the task remains unexplored for most other languages, mainly due to the lack of annotated datasets. This work presents a native QA dataset for an East African language, Tigrinya. The dataset contains 10.6K question-answer pairs spanning 572 paragraphs extracted from 290 news articles on various topics. The dataset construction method is discussed, which is applicable to constructing similar resources for related languages. We present comprehensive experiments and analyses of several resource-efficient approaches to QA, including monolingual, cross-lingual, and multilingual setups, along with comparisons against machine-translated silver data. Our strong baseline models reach 76{\%} in the F1 score, while the estimated human performance is 92{\%}, indicating that the benchmark presents a good challenge for future work. We make the dataset, models, and leaderboard publicly available.}
}

@misc{gaim-etal-2025-tiald,
  title    = {A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings},
  author   = {Fitsum Gaim and Hoyun Song and Huije Lee and Changgeon Ko and Eui Jun Hwang and Jong C. Park},
  year     = {2025},
  url      = {https://arxiv.org/abs/2505.12116},
  tasks    = {Hate/Abusiveness Detection, Sentiment Analysis, Topic Classification},
  abstract = {Content moderation research has recently made significant advances, but still fails to serve the majority of the world's languages due to the lack of resources, leaving millions of vulnerable users to online hostility. This work presents a large-scale human-annotated multi-task benchmark dataset for abusive language detection in Tigrinya social media with joint annotations for three tasks: abusiveness, sentiment, and topic classification. The dataset comprises 13,717 YouTube comments annotated by nine native speakers, collected from 7,373 videos with a total of over 1.2 billion views across 51 channels. We developed an iterative term clustering approach for effective data selection. Recognizing that around 64% of Tigrinya social media content uses Romanized transliterations rather than native Ge'ez script, our dataset accommodates both writing systems to reflect actual language use. We establish strong baselines across the tasks in the benchmark, while leaving significant challenges for future contributions. Our experiments reveal that small, specialized multi-task models outperform the current frontier models in the low-resource setting, achieving up to 86% accuracy (+7 points) in abusiveness detection. We make the resources publicly available to promote research on online safety.}
}

@inproceedings{gasser-etal-2011-hornmorpho,
  title     = {HornMorpho: a system for morphological processing of Amharic, Oromo, and Tigrinya},
  author    = {Gasser, Michael},
  booktitle = {Conference on Human Language Technology for Development, Alexandria, Egypt},
  year      = {2011},
  tasks     = {Morphological Processing},
  abstract  = {Despite its linguistic complexity, the Horn of Africa region includes several major languages with more than 5 million speakers, some crossing the borders of multiple countries. All of these languages have official status in regions or nations and are crucial for development; yet computational resources for the languages remain limited or non-existent. Since these languages are complex morphologically, software for morphological analysis and generation is a necessary first step toward nearly all other applications. This paper describes a resource for morphological analysis and generation for three of the most important languages in the Horn of Africa, Amharic, Tigrinya, and Oromo.}
}

@article{gebremeskel-etal-2023-hybrid-morphological-analyzer,
  title    = {Unlock Tigrigna NLP: Design and Development of Morphological Analyzer for Tigrigna Verbs Using Hybrid Approach},
  author   = {Hagos Gebremedhin Gebremeskel and Feng Chong and Huang Heyan},
  journal  = {SSRN Electronic Journal},
  year     = {2023},
  url      = {https://api.semanticscholar.org/CorpusID:265633133},
  tasks    = {Morphological Processing},
  abstract = {Morphological analyzer is the basis for various high-level NLP applications such as information retrieval, spell checking, grammar checking, machine translation, speech recognition, POS tagging and automatic sentence construction. This paper is carefully designed for the analysis of Tigrigna verbs morphological analyzer using the hybrid of memory learning and rule based approaches. The experiment have conducted using Python 3 where TiMBL algorithms IB2 and TRIBL2, and Finite State Transducer rules are used. The performance of the system has been evaluated using 10 fold cross-validation technique. Testing was conducted using optimized parameter settings for regular verbs and linguistic rules of the Tigrigna language allomorph and phonology for the irregular verbs. The accuracy of the memory based approach with optimized parameters of TiMBL algorithm IB2 and TRIBL2 was 93.24% and 92.31%, respectively. Finally, the hybrid approach had an actual performance of 95.6% using linguistic rules for handling irregular and copula verbs.}
}

@inproceedings{gedamu-2023-tigrinya-dialect,
  title     = {Tigrinya Dialect Identification},
  author    = {Asfaw Gedamu and Asmelash Teka Hadgu},
  booktitle = {AfricaNLP workshop at ICLR2023},
  year      = {2023},
  url       = {https://openreview.net/pdf?id=kiG8qiUFm2u},
  tasks     = {Language Identification},
  abstract  = {Dialect Identification is an important topic of research in Natural Language Processing (NLP) as it has broad implications in many real-world applications such as machine translation, speech recognition and chatbots to name a few. In this work, we investigate Tigrinya dialect identification using machine learning techniques. To that end, we have identified three Tigrinya dialects, namely: Z, L and D. Then we systematically collected datasets for each dialect. Finally, we perform experiments using classical machine learning and deep learning methods to quantify effectiveness of current methods on the problem of Tigrinya dialect identification. The highest overall accuracy of 92.98% was achieved using character-level Convolutional Neural Networks (CNNs).}
}

@inproceedings{hadgu-etal-2022-lesan,
  title        = {Lesan--machine translation for low resource languages},
  author       = {Hadgu, Asmelash Teka and Aregawi, Abel and Beaudoin, Adam},
  booktitle    = {NeurIPS 2021 Competitions and Demonstrations Track},
  year         = {2022},
  organization = {PMLR},
  pages        = {297--301},
  url          = {https://proceedings.mlr.press/v176/hadgu22a/hadgu22a.pdf},
  tasks        = {Machine Translation},
  abstract     = {Millions of people around the world can not access content on the Web because most of the content is not readily available in their language. Machine translation (MT) systems have the potential to change this for many languages. Current MT systems provide very accurate results for high resource language pairs, e.g., German and English. However, for many low resource languages, MT is still under active research. The key challenge is lack of datasets to build these systems. We present Lesan (https://lesan.ai/), an MT system for low resource languages. Our pipeline solves the key bottleneck to low resource MT by leveraging online and offline sources, a custom Optical Character Recognition (OCR) system for Ethiopic and an automatic alignment module. The final step in the pipeline is a sequence to sequence model that takes parallel corpus as input and gives us a translation model. Lesan{’}s translation model is based on the Transformer architecture. After constructing a base model, back translation is used to leverage monolingual corpora. Currently Lesan supports translation to and from Tigrinya, Amharic and English. We perform extensive human evaluation and show that Lesan outperforms state-of-the-art systems such as Google Translate and Microsoft Translator across all six pairs. Lesan is freely available and has served more than 10 million translations so far. At the moment, there are only 217 Tigrinya and 15,009 Amharic Wikipedia articles. We believe that Lesan will contribute towards democratizing access to the Web through MT for millions of people.}
}

@inproceedings{hailu-etal-2024-tigrinya-ocr,
  title     = {Tigrinya OCR: Applying CRNN for Text Recognition},
  author    = {Hailu, Aaron Afewerki and Hayleslassie, Abiel Tesfamichael and Gebresilasie, Danait Weldu and Haile, Robel Estifanos and Ghebremedhin, Tesfana Tekeste and Tedla, Yemane Keleta},
  booktitle = {Neural Information Processing},
  year      = {2024},
  publisher = {Springer Nature Singapore},
  pages     = {456--467},
  address   = {Singapore},
  isbn      = {978-981-99-8184-7},
  tasks     = {Optical Character Recognition},
  editor    = {Luo, Biao and Cheng, Long and Wu, Zheng-Guang and Li, Hongyi and Li, Chaojie}
}

@article{isayas-2021-nmt,
  title    = {A First Look into Neural Machine Translation for Tigrinya},
  author   = {Adhanom, Isayas Berhe},
  journal  = {arXiv},
  year     = {2021},
  tasks    = {Machine Translation},
  abstract = {This paper presents a study that investigates the use of current neural machine translation (NMT) techniques for Tigrinya - a low-resourced Semitic language. The proposed method achieves state-of-the-art performance on the task of English-Tigrinya translation using the JW300 En-Ti dataset. The study uses the transformer architecture to achieve translation performance that improves over previous techniques. Additionally, an evaluation dataset and a monolingualTigrinya corpus are built as part of the study and are made available for public use. We also evaluate the effectiveness of back-translation for NMT in Tigrinyaby adding more than a million sentences, generated by back-translation from our monolingual corpus, to our small-sized parallel corpus. By releasing these resources in addition to the code and the results produced in this study, this study aims to provide a benchmark for other researchers in Tigrinya machine translation to compare and build upon.}
}

@article{kidane-etal-2021-augmentation,
  title    = {An Exploration of Data Augmentation Techniques for Improving English to Tigrinya Translation},
  author   = {Lidia Kidane and Sachin Kumar and Yulia Tsvetkov},
  journal  = {ArXiv},
  year     = {2021},
  volume   = {abs/2103.16789},
  tasks    = {Machine Translation},
  abstract = {It has been shown that the performance of neural machine translation (NMT) drops starkly in low-resource conditions, often requiring large amounts of auxiliary data to achieve competitive results. An effective method of generating auxiliary data is back-translation of target language sentences. In this work, we present a case study of Tigrinya where we investigate several back-translation methods to generate synthetic source sentences. We find that in low-resource conditions, back-translation by pivoting through a higher-resource language related to the target language, proves most effective resulting in substantial improvements over baselines.}
}

@inproceedings{littell-etal-2018-parser,
  title     = {Parser combinators for {T}igrinya and {O}romo morphology},
  author    = {Littell, Patrick  and McCoy, Tom  and Han, Na-Rae  and Rijhwani, Shruti  and Sheikh, Zaid  and Mortensen, David  and Mitamura, Teruko  and Levin, Lori},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},
  month     = {May},
  year      = {2018},
  publisher = {European Language Resources Association (ELRA)},
  address   = {Miyazaki, Japan},
  url       = {https://aclanthology.org/L18-1611},
  tasks     = {Morphological Processing},
  abstract  = {We present rule-based morphological parsers in the Tigrinya and Oromo languages, based on a parser-combinator rather than finite-state paradigm. This paradigm allows rapid development and ease of integration with other systems, although at the cost of non-optimal theoretical efficiency. These parsers produce multiple output representations simultaneously, including lemmatization, morphological segmentation, and an English word-for-word gloss, and we evaluate these representations as input for entity detection and linking and humanitarian need detection.}
}

@inproceedings{ogueji-etal-2021-small,
  title     = {Small Data? No Problem! Exploring the Viability of Pretrained Multilingual Language Models for Low-resourced Languages},
  author    = {Ogueji, Kelechi and Zhu, Yuxin and Lin, Jimmy},
  booktitle = {Proceedings of the 1st Workshop on Multilingual Representation Learning},
  month     = {November},
  year      = {2021},
  publisher = {Association for Computational Linguistics},
  pages     = {116--126},
  address   = {Punta Cana, Dominican Republic},
  doi       = {10.18653/v1/2021.mrl-1.11},
  url       = {https://aclanthology.org/2021.mrl-1.11/},
  tasks     = {Language Modeling},
  abstract  = {Pretrained multilingual language models have been shown to work well on many languages for a variety of downstream NLP tasks. However, these models are known to require a lot of training data. This consequently leaves out a huge percentage of the world’s languages as they are under-resourced. Furthermore, a major motivation behind these models is that lower-resource languages benefit from joint training with higher-resource languages. In this work, we challenge this assumption and present the first attempt at training a multilingual language model on only low-resource languages. We show that it is possible to train competitive multilingual language models on less than 1 GB of text. Our model, named AfriBERTa, covers 11 African languages, including the first language model for 4 of these languages. Evaluations on named entity recognition and text classification spanning 10 languages show that our model outperforms mBERT and XLM-Rin several languages and is very competitive overall. Results suggest that our “small data” approach based on similar languages may sometimes work better than joint training on large datasets with high-resource languages. Code, data and models are released at https://github.com/keleog/afriberta.},
  editor    = {Ataman, Duygu and Birch, Alexandra and Conneau, Alexis and Firat, Orhan and Ruder, Sebastian and Sahin, Gozde Gul}
}

@inproceedings{oktem-etal-2020-mt-transfer,
  title     = {Tigrinya Neural Machine Translation with Transfer Learning for Humanitarian Response},
  author    = {Alp {\"{O}}ktem and Mirko Plitt and Grace Tang},
  booktitle = {1st AfricaNLP Workshop Proceedings, AfricaNLP@ICLR 2020, Virtual Conference},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.11523},
  tasks     = {Machine Translation},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-11523.bib},
  editor    = {Kathleen Siminyu and Laura Martinus and Vukosi Marivate},
  timestamp = {Wed, 06 Sep 2023 09:56:29 +0200}
}

@inproceedings{osman-mikami-2012-stemming,
  title     = {Stemming {T}igrinya Words for Information Retrieval},
  author    = {Osman, Omer and Mikami, Yoshiki},
  booktitle = {Proceedings of {COLING} 2012: Demonstration Papers},
  month     = {December},
  year      = {2012},
  publisher = {The COLING 2012 Organizing Committee},
  pages     = {345--352},
  address   = {Mumbai, India},
  url       = {https://aclanthology.org/C12-3043},
  tasks     = {Morphological Processing, Information Retrieval},
  abstract  = {The increasing penetration of internet into less developed countries has resulted in the increase in the number of digital documents written in many minor languages. However, many of these languages have limited resources in terms of data, language resources and computational tools. Stemming is the reduction of inflected word forms into common basic form. It is an important analysis process in information retrieval and many natural language processing applications. In highly inflected languages such as Tigrinya, stemming is not always straightforward task. In this paper we present the development of stemmer for Tigrinya words to facilitate the information retrieval. We used a hybrid approach for stemming that combines rule based stemming which removes affixes in successively applied steps and dictionary based stemming which reduces stemming errors by verifying the resulting stem based on word distance measures. The stemmer was evaluated using two sets of Tigrinya words. The results show that it achieved an average accuracy of 89.3\%.}
}

@inproceedings{sewunetie-etal-2024-gender-bias,
  title     = {Gender Bias Evaluation in Machine Translation for {A}mharic, {T}igrigna, and Afaan Oromoo},
  author    = {Sewunetie, Walelign  and Tonja, Atnafu  and Belay, Tadesse  and Nigatu, Hellina Hailu  and Gebremeskel, Gashaw  and Mossie, Zewdie  and Seid, Hussien  and Yimam, Seid},
  booktitle = {Proceedings of the 2nd International Workshop on Gender-Inclusive Translation Technologies},
  month     = {June},
  year      = {2024},
  publisher = {European Association for Machine Translation (EAMT)},
  pages     = {1--11},
  address   = {Sheffield, United Kingdom},
  url       = {https://aclanthology.org/2024.gitt-1.1/},
  tasks     = {Machine Translation},
  abstract  = {While Machine Translation (MT) research has progressed over the years, translation systems still suffer from biases, including gender bias. While an active line of research studies the existence and mitigation strategies of gender bias in machine translation systems, there is limited research exploring this phenomenon for low-resource languages. The limited availability of linguistic and computational resources confounded with the lack of benchmark datasets makes studying bias for low-resourced languages that much more difficult. In this paper, we construct benchmark datasets to evaluate gender bias in machine translation for three low-resource languages: Afaan Oromoo (Orm), Amharic (Amh), and Tigrinya (Tir). Building on prior work, we collected 2400 gender-balanced sentences parallelly translated into the three languages. From human evaluations of the dataset we collected, we found that about 93{\%} of Afaan Oromoo, 80{\%} of Tigrinya, and 72{\%} of Amharic sentences exhibited gender bias. In addition to providing benchmarks for improving gender bias mitigation research in the three languages, we hope the careful documentation of our work will help other low-resourced language researchers extend our approach to their languages.},
  editor    = {Savoldi, Beatrice  and Hackenbuchner, Jani{\c{c}}a  and Bentivogli, Luisa  and Daems, Joke  and Vanmassenhove, Eva  and Bastings, Jasmijn}
}

@article{tedla-etal-2016-pos-tagging,
  title    = {Tigrinya Part-of-Speech Tagging with Morphological Patterns and the New Nagaoka Tigrinya Corpus},
  author   = {Yemane Tedla and Kazuhide Yamamoto and A. Marasinghe},
  journal  = {International Journal of Computer Applications},
  year     = {2016},
  pages    = {33-41},
  volume   = {146},
  tasks    = {Part of Speech Tagging},
  abstract = {This paper presents the first part-of-speech (POS) tagging research for Tigrinya (Semitic language) from the newly constructed Nagaoka Tigrinya Corpus. The raw text was extracted from a newspaper published in Eritrea in the Tigrinya language. This initial corpus was cleaned and formatted in plaintext and the Text Encoding Initiative (TEI) XML format. A tagset of 73 tags was designed, and the corpus for POS was manually annotated. This tagset encompasses three levels of grammatical information, which are the main POS categories, subcategories, and POS clitics. The POS tagged corpus contains 72,080 tokens. Tigrinya has a unique pattern of root-template morphology that can be utilized to infer POS categories. Subsequently, a supervised learning approach based on conditional random fields (CRFs) and support vector machines (SVMs) was applied, trained over contextual features of words and POS tags, morphological patterns, and affixes. A rigorous parameter optimization was performed and different combinations of features, data size, and tagsets were experimented upon to boost the overall accuracy, and particularly the prediction of POS for unknown words. For a reduced tagset of 20 tags, an overall accuracy of 90.89\% was obtained on a stratified 10fold cross validation. Enriching contextual features with morphological and affix features improved performance up to 41.01 percentage point, which is significant.}
}

@inproceedings{tedla-etal-2016-segmentation-mt,
  title        = {The effect of shallow segmentation on English-Tigrinya statistical machine translation},
  author       = {Tedla, Yemane and Yamamoto, Kazuhide},
  booktitle    = {2016 International Conference on Asian Language Processing (IALP)},
  year         = {2016},
  organization = {IEEE},
  pages        = {79--82},
  tasks        = {Machine Translation, Morphological Processing},
  abstract     = {This paper presents initial research on English-to-Tigrinya statistical machine translation (SMT). Tigrinya is a highly inflected Semitic language spoken in Eritrea and Ethiopia. Translation involving morphologically complex languages is challenged by factors including data sparseness, word alignment and language model. We try to address these problems through morphological segmentation of Tigrinya words. As a result of segmentation, the size of the language model and its perplexity were greatly reduced. Furthermore, the increase in Tigrinya tokens decreased out-of-vocabulary ratio by 46\%. We analysed phrase-based translation with unsegmented and segmented corpus to investigate the effect of segmentation on translation quality. Preliminary results demonstrate promising performance improvement from a relatively small parallel corpus.}
}

@article{tedla-etal-2017-analyzing-we,
  title    = {Analyzing Word Embeddings and Improving POS Tagger of Tigrinya},
  author   = {Yemane Tedla and Kazuhide Yamamoto},
  journal  = {2017 International Conference on Asian Language Processing (IALP)},
  year     = {2017},
  pages    = {115-118},
  tasks    = {Part of Speech Tagging, Word Embedding},
  abstract = {In this paper, we analyze word embeddings for a morphologically rich language, Tigrinya. Tigrinya is a Semitic language spoken natively in Eritrea and Ethiopia by over seven million people. The unique and complex morphology of Semitic languages, which includes Arabic, Amharic, and Hebrew, is commonly known as 'root and template pattern' morphology. This morphology generates a large number of inflected forms that often cause out-of-vocabulary (OOV) challenges in language processing. This problem is more challenging for low resource languages, such as Tigrinya, that offers very little support of annotated resources. Word embedding methods, given a large raw text corpus, form semantic and syntactic vector representation of words. Therefore, we construct a new text corpus and investigate the optimal settings for generating word vectors for Tigrinya. We also utilize word embeddings to improve the performance of a Tigrinya part-of-speech tagger created from a small tagged corpus.}
}

@inproceedings{tedla-etal-2018-lstm-segmentation,
  title     = {Morphological Segmentation with LSTM Neural Networks for Tigrinya},
  author    = {Yemane Tedla and Kazuhide Yamamoto},
  booktitle = {Intenational Journal on Natural Language Computing (JNLC)},
  journal   = {Intenational Journal on Natural Language Computing (JNLC)},
  year      = {2018},
  volume    = {7},
  tasks     = {Morphological Processing},
  abstract  = {Morphological segmentation is a fundamental task in language processing. Some languages, such as Arabic and Tigrinya, have words packed with very rich morphological information. Therefore, unpacking this information becomes a necessary task for many downstream natural language processing tasks. This paper presents the first morphological segmentation research for Tigrinya. We constructed a new morphologically segmented corpus with about 45,127 manually segmented tokens. Conditional random fields (CRF) and window-based long short-term memory (LSTM) neural networks were employed separately to develop our boundary detection models. We applied language-independent character and substring features for the CRF and character embeddings for the LSTM networks. Experiments were performed with four variants of the Begin-Inside-Outside (BIO) chunk annotation scheme. We achieved 94.67\% F1 score using bidirectional LSTMs with window approach to morpheme boundary detection.}
}

@inproceedings{teklehaymanot-etal-2024-tigqa,
  title     = {{TIGQA}: An Expert-Annotated Question-Answering Dataset in {T}igrinya},
  author    = {Teklehaymanot, Hailay Kidu and Fazlija, Dren and Ganguly, Niloy and Patro, Gourab Kumar and Nejdl, Wolfgang},
  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  month     = {May},
  year      = {2024},
  publisher = {ELRA and ICCL},
  pages     = {16142--16161},
  address   = {Torino, Italia},
  url       = {https://aclanthology.org/2024.lrec-main.1404/},
  tasks     = {Question Answering},
  editor    = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen}
}

@misc{tela-etal-2020-transfer-monolingual-model,
  title         = {Transferring Monolingual Model to Low-Resource Language: The Case of Tigrinya},
  author        = {Abrhalei Tela and Abraham Woubie and Ville Hautamaki},
  year          = {2020},
  primaryclass  = {cs.CL},
  eprint        = {2006.07698},
  archiveprefix = {arXiv},
  tasks         = {Language Modeling, Sentiment Analysis},
  abstract      = {In recent years, transformer models have achieved great success in natural language processing (NLP) tasks. Most of the current state-of-the-art NLP results are achieved by using monolingual transformer models, where the model is pre-trained using a single language unlabelled text corpus. Then, the model is fine-tuned to the specific downstream task. However, the cost of pre-training a new transformer model is high for most languages. In this work, we propose a cost-effective transfer learning method to adopt a strong source language model, trained from a large monolingual corpus to a low-resource language. Thus, using XLNet language model, we demonstrate competitive performance with mBERT and a pre-trained target language model on the cross-lingual sentiment (CLS) dataset and on a new sentiment analysis dataset for low-resourced language Tigrinya. With only 10k examples of the given Tigrinya sentiment analysis dataset, English XLNet has achieved 78.88\% F1-Score outperforming BERT and mBERT by 10\% and 7\%, respectively. More interestingly, fine-tuning (English) XLNet model on the CLS dataset has promising results compared to mBERT and even outperformed mBERT for one dataset of the Japanese language.},
  tldr          = {This work proposes a cost-effective transfer learning method to adopt a strong source language model, trained from a large monolingual corpus to a low-resource language, and demonstrates competitive performance with mBERT and a pre-trained target language model on the cross-lingual sentiment (CLS) dataset and on a new sentiment analysis dataset for low-resourced language Tigrinya.}
}

@article{tesfagergish-2020-pos-dnn,
  title    = {Part-of-Speech Tagging via Deep Neural Networks for Northern-Ethiopic Languages},
  author   = {Senait Gebremichael Tesfagergish and Jurgita Kapociute-Dzikiene},
  journal  = {Information Technology and Control},
  year     = {2020},
  pages    = {482-494},
  volume   = {49},
  tasks    = {Part of Speech Tagging},
  abstract = {Deep Neural Networks (DNNs) have proven to be especially successful in the area of Natural Language Processing (NLP) and Part-Of-Speech (POS) tagging—which is the process of mapping words to their corresponding POS labels depending on the context. Despite recent development of language technologies, low-resourced languages (such as an East African Tigrinya language), have received too little attention. We investigate the effectiveness of Deep Learning (DL) solutions for the low-resourced Tigrinya language of the Northern-Ethiopic branch. We have selected Tigrinya as the testbed example and have tested state-of-the-art DL approaches seeking to build the most accurate POS tagger. We have evaluated DNN classifiers (Feed Forward Neural Network – FFNN, Long Short-Term Memory method – LSTM, Bidirectional LSTM, and Convolutional Neural Network – CNN) on a top of neural word2vec word embeddings with a small training corpus known as Nagaoka Tigrinya Corpus. To determine the best DNN classifier type, its architecture and hyper-parameter set both manual and automatic hyper-parameter tuning has been performed. BiLSTM method was proved to be the most suitable for our solving task: it achieved the highest accuracy equal to 92\% that is 65\% above the random baseline.}
}

@mastersthesis{tsegaye-2014-factored-mt,
  title    = {English-Tigrigna Factored Statistical Machine Translation},
  author   = {Tarik Tsegaye},
  journal  = {Master's Thesis, submitted to Addis Ababa University},
  year     = {2014},
  school   = {Addis Ababa University},
  tasks    = {Machine Translation},
  abstract = {A thesis submitted to the school of graduate studies of Addis Ababa University in partial fulfillment of the requirements of the degree of Master of Science in Information Science}
}

@inproceedings{yohannes-etal-2022-ner,
  title     = {Named-Entity Recognition for a Low-Resource Language Using Pre-Trained Language Model},
  author    = {Yohannes, Hailemariam Mehari and Amagasa, Toshiyuki},
  booktitle = {Proceedings of the 37th SIGAPP Symposium on Applied Computing},
  year      = {2022},
  keywords  = {pre-trained language model, POS tagging, name entity recognition, RoBERTa, low-resource language},
  location  = {Virtual Event},
  numpages  = {8},
  publisher = {Association for Computing Machinery},
  series    = {SAC '22},
  pages     = {837–844},
  address   = {New York, NY, USA},
  doi       = {10.1145/3477314.3507066},
  isbn      = {9781450387132},
  url       = {https://doi.org/10.1145/3477314.3507066},
  tasks     = {Named Entity Recognition},
  abstract  = {This paper proposes a method for Named-Entity Recognition (NER) for a low-resource language, Tigrinya, using a pre-trained language model. Tigrinya is a morphologically rich language, although one of the underrepresented in the field of NLP. This is mainly due to the limited amount of annotated data available. To address this problem, we introduced the first publicly available NER dataset for Tigrinya. The dataset contains 69,309 tokens that were manually annotated based on the CoNLL 2003 Beginning, Inside, and Outside (BIO) tagging schema. Specifically, we develop a new pre-trained language model for Tigrinya based on RoBERTa, which we refer to as TigRoBERTa. First, It is trained on an unsupervised Tigrinya corpus using Masked Language Modeling (MLM). Then, we show the validity of TigRoBERTa by fine-tuning for a couple of downstream tasks, namely, NER and Part of Speech (POS) tagging. The experimental results show that the method achieved 81.05\% F1-score for NER and 92\% accuracy for POS tagging, which is better than or comparable to the baseline method based on the CNN-BiLSTM-CRF model.}
}
